---
title             : "Crossing a River to get some Water? An Empirical Comparison of Classic and Contemporary Approaches to Item Social Desirability Evaluation"
shorttitle        : "ITEM SD RATINGS"

author: 
  - name          : "John T. Kulas"
    affiliation   : "1"
    corresponding : yes
    address       : "250 Dickson Hall; Montclair State University; Montclair, NJ, 07043"
    email         : "jtkulas@ergreports.com"
  - name          : "Emily J. Johnson"
    affiliation   : "2"
    email         : "ejjohnson@stcloudstate.edu"
  - name          : "Renata GarcÃ­a Prieto Palacios Roji"
    affiliation   : "3"
    email         : "garciaprier1@montclair.edu"
  - name          : "Julia Wefferling"
    affiliation   : "3"
    email         : "wefferlingj1@montclair.edu"
        
affiliation:
  - id            : "1"
    institution   : "eRg"
  - id            : "2"
    institution   : "St. Cloud State University"
  - id            : "3"
    institution   : "Montclair State University"

author_note: |
  John T. Kulas, President, eRg. Emily J. Johnson, Research Assistant, St. Cloud State University. Julia Wefferling, Research Assistant, Montclair State University. Renata Garcia Prieto Palacios Roji, Research Assistant, Montclair State University. This paper was generated via papaja [@R-papaja].


abstract: |

  Traditional approaches to the assessment of socially desirable content within Psychological inventory indicators have been implicated as being too broadly focused. Correspondingly, an alternative method has been proposed whereby the target of rating is shifted from the item *stem* to the item's *response option* [@kuncel_conceptual_2009]. The current study examines whether the added complexity of the more contemporary procedure is accompanied with an incrementally meaningful amount of unique information regarding the magnitude and valence of socially desirable content within Psychological inventory indicators. Toward this pursuit, the historically traditional and more recently advocated methodologies were empirically compared and contrasted. Our interest was in collecting estimates of: 1) similarity (and uniqueness) of information, 2) inter-rater consistency (when making evaluations), and 3) cognitive difficulty of the rating processes. Results suggest that although the contemporary approach captures some unique information, this is in fact only incrementally informative in predictably particular instances. Specifically, the more cognitively taxing contemporary procedure may be best leveraged with indicators first implicated as "moderately desirable" via application of the traditional [@edwards_relationship_1953; @edwards_social_1957] approach.  A more complementary application of the two approaches should benefit both researchers and item judges.
  
  >**Yet to do 3/11/23**: 1) graph relating Edwards to K/T (maybe look at residuals instead of subjective ratings), 2) response latencies (proxy for task difficulty), 3) inter-rater agreement (also proxy for difficulty of task)
  
keywords          : "Social desirability, response bias, personality assessment, content validation"
#wordcount         : "X"

bibliography      : [temp.bib, packages.bib, KuncelTellegen.bib, emily.bib]
csl: "apa7.csl"
figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "jou"
header-includes:
- \usepackage{pdflscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
output            : papaja::apa6_pdf


#header-includes: #used to change page orientation
#- \usepackage{pdflscape}
#- \newcommand{\blandscape}{\begin{landscape}}
#- \newcommand{\elandscape}{\end{landscape}}
---

```{r load_packages, include = FALSE}

library(papaja)   # APA6 formatting
#library(citr)     # APAcitation
library(psych)
library(ggplot2)  # data visualization
library(descr)
library(magick)
library(grid)     # grid.raster (getting figure in papaja)
library(careless) # careless responding indices- irv and longstring

knitr::write_bib(.packages(all.available = TRUE), "packages.bib")

```

It may perhaps be adaptive human nature to possess an overly positive evaluation of oneself [@alicke_handbook_2011; @alicke_self-enhancement_2009; @sedikides_self-enhancement_2012; @taylor_illusion_1988]. However, different contexts are also known to either prime [@birkeland_meta-analytic_2006; @donovan_impact_2014; @morgeson_reconsidering_2007] or potentially suppress such positive bias in self-evaluation - such as, for example, when accuracy is deemed important [e.g., @dauenheimer_self-enhancement_2002]. In particular, individuals may feel compelled to present themselves in a favorable manner (possibly inconsistent with their own true character) in situations that pose high-stakes consequences, such as a job interview [e.g., @barrick_what_2009; @levashina_model_2006; @weiss_looking_2006] or attempting to attract a potential mate [e.g., @dimoulas_patterns_1998]. When applied to the domain of Psychological assessment, these proclivities are generally contextualized as acts consistent with a *socially desirable* response orientation, and reflect an individual's endorsement of characteristics that are culturally valued or desired rather than what may be objectively true of the person him or herself [@ziegler_applicant_2011; @kuncel_conceptual_2009].  

Procedurally, these response tendencies within Psychological assessment contexts have been most commonly examined via experimental priming [for example, instructions to "fake" or respond honestly, @birkeland_meta-analytic_2006], identification of populations assumed to have divergent response motives [for example, comparisons of job applicant versus non-applicant samples, @viswesvaran_meta-analyses_1999], or assessment of individual differences in likelihood of responding in a socially desirable manner [for example, @li_using_2006]. Less common in contemporary investigations of social desirability are protocols that directly measure and evaluate the saturation of socially desirable (or undesirable) content within inventory indicators themselves.  

These indicator saturation investigations *did* enjoy a brief flurry of attention in the mid $20^{th}$ Century [see, for example, @edwards_relationship_1953; @edwards_social_1957; @edwards_social_1957-4], although this interest dimmed without the direct advocacy of its originating proponent and researcher, Allen Edwards. Recently, there has been a little movement toward revisiting these direct item evaluations [@leising2021correlations; @cui2022distinguishing], as well as a contemporary recommendation aimed at the *method* used to collect the evaluations [aka "ratings", e.g., @kuncel_conceptual_2009]. The current paper contrasts the traditional (aka "Edwardian") with the more recently advocated contemporary methodology. Our intent was to investigate possible redundancies in information conveyed across the two approaches, as well as to seek out indicators of task complexity when judges are asked to provide such ratings. 

## The Role of Social Desirability in Psychological Assessment 

Two contemporary methodologies have been most commonly applied in the evaluation of social desirability's impact on Psychological assessment scores, and both generally support the conclusion that social desirability should not be considered overly problematic [e.g., it is a "red herring", @ones_role_1996].  

The first popular contemporary methodology involves assessing individual differences in socially desirable response tendencies via questionnaire administration. These differences in social desirable tendencies can then be leveraged to partial out social desirability effects via covariate specification - for example in the context of assessment validation. Historically popular measures used in this application include, for example,  the Balanced Inventory of Desirable Responding (BIDR), or the Marlowe-Crowne Social Desirability Scale [e.g., see @crowne_new_1960; @li_using_2006; @paulhus_balanced_1988]. 

The second set of popular contemporary methodologies employs either experimental instructions to "fake" responses or comparisons of job applicant versus non-applicant respondents [e.g., @birkeland_meta-analytic_2006; @viswesvaran_meta-analyses_1999]. Patterns of response are then investigated under conditions thought to be susceptible to socially desirable responding (e.g., fake experimental conditions or applicant respondent samples) versus conditions purported to be lacking socially desirable influence (e.g., control or honest response honest experimental conditions, and non-applicant respondents). 

The meta-analyses of @birkeland_meta-analytic_2006, @ones_role_1996, and @viswesvaran_meta-analyses_1999 summarize findings across studies leveraging each of these common approaches. @ones_role_1996, for example, investigated individual differences in socially desirable responding tendencies as assessed via individual difference measures such as the BIDR, and used this information to construct semipartial correlations between Big 5 scales and work-relevant criteria (e.g., training performance, counterproductive behaviors, job performance). Using this statistical methodology, @ones_role_1996 noted little effect of socially desirable response tendencies on criterion-related validities (the semi-partial correlations were similar in magnitude to uncorrected coefficients).

@viswesvaran_meta-analyses_1999 applied a similar meta-analytic lens to *experimental* investigations involving instructions to "fake good" or "fake bad", finding that Big 5 scales tended to exhibit similar levels of fakability. This analysis confirmed that respondents can indeed intentionally distort their responses (e.g., respond in a socially desirable manner) if instructed to do so. Regarding non-laboratory investigations where context is assumed to prime a socially desirable response orientation, @birkeland_meta-analytic_2006 similarly documented elevated Big 5 scale scores with applicant respondents relative to non-applicant respondents, but also noted that the pattern of rating elevation differed across the type of position the applicant was seeking. Note here that all methodologies encompassed by these meta-analyses are characterized by an individual difference orientation (e.g., it is differences across respondent proclivity to enhance - either driven by context or psyche - within which the social desirability influence is manifest).  

### An Elemental Focus Alternative

Alternative to the above-noted inter-individual-oriented approaches to exploring social desirability's role in Psychological assessment, there exists a subset of researchers who have focused on the assessment elements themselves [e.g., the *item*, see, for example, @edwards_social_1957]. This approach appears to be more contemporarily popular within non-work assessment domains than the business or Industrial and Organizational assessment literatures [see, for example, @leising_vocabulary_2012; @leising_model_2015].  

For roughly 60 years, the standard investigation of item-level saturation with socially desirable content had been applied in a fairly consistent manner, with little methodological deviation from the procedure first advocated by Edwards. @edwards_relationship_1953 simply asked judges to rate the content of personality items along a social desirability continuum (wherein, for example, the personality item, "I hate people" would likely be deemed less desirable than an item such as, "I regularly give money to charities in need"). Edwards specifically asked his judges to provide ratings ranging from extremely undesirable to extremely desirable along a 9-point scale, and subsequently went on to further demonstrate that the more socially desirable an item is, the more likely someone will endorse having that characteristic [@edwards_relationship_1953; @edwards_social_1957] [^1]. 

[^1]: This is a very robust finding that has been replicated many times. The implications of this finding are also far-reaching, and constitute one of the reasons an exploration of the contemporary viability of Edwards' approach is deemed important. However, the focus of the current exploration is fully *procedural*, pointed directly at the *method used to collect item social desirability ratings* rather than the broader implications of attraction toward the socially desirable within Psychological assessment. 

#### A Procedural Revisitation

@kuncel_conceptual_2009 revisited the Edwardian item rating protocol, proposing that traditional measurement approaches such as Edwards' are perhaps overly simplistic if assessment specialists aim to truly understand the impact of social desirability on assessment responses. Specifically, @kuncel_conceptual_2009 noted that previous investigations had largely ignored the potential for social desirability to be maximally salient at locations other than trait extremes. This perspective challenged the previously implicit assumption that social desirability manifests itself in a linear fashion across response options, whereby "agreement" with more (or less) of a characteristic is consistently associated with greater levels of social desirability (or *un*desirability). Procedurally, @kuncel_conceptual_2009 assessed differential attraction to item *response options*, as opposed to the Edwardian focus on the *item stem*.  	

As rightly noted by @kuncel_conceptual_2009, there are plausible characteristics with a *most* desirable standing location that is not located at either extreme (consider, for example, "being quiet" - it is, without additionally provided context, likely most socially desirable to be moderate along the trait continuum for this characteristic). As an explicit alternative to the implicit Edwardian assumption of linear social desirability manifestation, @kuncel_conceptual_2009 proposed that at least four patterns of item social desirability may commonly exist across scaled inventory response options: linear, non-linear monotonic (rate of increase is not constant), weakly non-linear monotonic (flat regions exist), and non-monotonic (pattern reversal).  

These possibilities acknowledge that the two-dimensional functional progression between an x-axis "location of response" (e.g., low, moderate, or high on the trait) and a y-axis "how desirable the location is" could be linear, logarithmic/exponential, flat in regions, or perhaps even "U"- or inverted "U"-shaped. The authors even suggested that *most* trait items may be best characterized by nonmonotonic or weakly monotonic relationships with social desirability and that a strictly linear relationship would be dependent on highly valued items or strongly incentivized contexts (for example, applying for a desired job). 

To test their premise, @kuncel_conceptual_2009 constructed an alternative rating system. This approach asks individual judges to rate items on *how desirable* (they deem) *the trait to be at five different levels* of the characteristic: extremely high (top 1%), above average (top 30%), average, below average (bottom 30%), or extremely low (bottom 1%; see Figure \@ref(fig:Figure1), which has been reproduced from the original @kuncel_conceptual_2009 publication). Note here that these five categories parallel the ubiquitous 5-point rating system often retained in self-report inventories. Applying this rating procedure, @kuncel_conceptual_2009 found support for their premise that not all items demonstrate linear associations with social desirability and that non-monotonic relationships do exist across graded response continua.

@kuncel_conceptual_2009's second study was designed to approximate real-world contexts. Here, participants were asked to act as if though they were in a pre-employment assessment situation and to explain their rationale when an extreme response was *not* chosen on the assessment. This design was intended to provide insight regarding the lack of linear manifestations of social desirability. @kuncel_conceptual_2009 found that, across administrations, over 60% of participants did in fact choose the most extreme response options. Some of the participants who opted out of endorsing the extreme responses, however, noted that the extreme response might be poorly perceived by an evaluator (i.e., too inaccurate, bragging, too good). Taken collectively, these investigations supported the notion that trait characteristics do not necessarily manifest only strictly linear associations with social desirability.

```{r Figure1, echo=FALSE, fig.cap="Kuncel and Tellegen (2009) protocol for determining socially desirable saturation at the item response level."}


figure <- image_read("Kuncel.PNG")

grid.raster(figure)


#@kuncel_conceptual_2009, @edwards_social_1957, @edwards_relationship_1953, @backstrom_social_2013
#@backstrom_social_2013 propose a method different from both @kuncel_conceptual_2009 and @edwards_relationship_1953 - 7/29/23: not really - this is the "neutralization" study

```

Although there is both theoretical and empirical support for  @kuncel_conceptual_2009's procedure, it also quite substantially more time- and (we propose) cognitive effort-intensive than is the traditional item-rating approach [@edwards_social_1957; @edwards_relationship_1953]. As technically specified, the traditional Edwards procedure requires one evaluation per item (albeit that evaluation is made across nine gradiated social desirability strata). The contemporary "Kuncel and Tellegen" procedure requires (in the case of 5-point Likert-type indicators) five evaluations across five levels of desirability per item.  In addition to the greater *number* of evaluations required in the contemporary approach, we propose that the contemporary approach is also likely more cognitively demanding due to shifting objects of reference (the referent of appraisal shifts across ratings - top 1%, top 30%, etc.).  

Given the greater time and resource commitments required of the contemporary approach relative to the traditional, we aim to gauge to what extent these two approaches in fact capture similar versus unique pieces of information. The goal of the present investigation is therefore to directly compare these two methodologies with an "additional information" orientation - that is, is the new approach truly unique, or rather does it at least with some indicators convey similar information as the classic, less cognitively taxing and more time-efficient approach?

>*Research Question 1*: Do the contemporary and traditional rating procedures capture unique information regarding social desirability saturation?

>*Research Question 2*: Is the contemporary procedure more cognitively taxing than the traditional procedure? $\leftarrow$ **Reword after finanlize analyses - didn't collect response latencies from the Edwards form**

# Study 1

# Methods
	
## Participants

Seventy-six undergraduate students made ratings of *either* item social desirability [*n* = 14, @edwards_social_1957], or levels of desirability associated with different trait levels [*n* = 62, e.g., @kuncel_conceptual_2009].

## Materials

The IPIP-NEO is a 300-item personality measure intended to assess the Big Five personality dimensions: Conscientiousness, Agreeableness, Extraversion, Openness to Experience, and Neuroticism (Johnson, 2005). For the purposes of the current investigation, we did not collect typical responses to these 300 indicators, but were rather interested in the evaluative content of the items (or, alternatively, the evaluative content associated with differential standing along the construct implied by the item response options).

## Procedure

All ratings were made via paper and pencil in an experimental laboratory. The @edwards_social_1957 ratings were made along Edwards' originally specified 9-point scale ranging from Extremely Undesirable to Extremely Desirable. Because we investigated a fairly large instrument, we constructed 2 counterbalanced "Edwards" forms as an effort to limit potential fatigue effects across the rating process. The @kuncel_conceptual_2009 ratings were collected from 60 different item stems across 10 different counterbalancings. Each rater (regardless of task; item stem or response option rating) was therefore asked to perform 300 total ratings (either 1 evaluation per 300 items or 5 evaluations per 60 items).
	
# Results  

All analyses were performed in R version `r paste0(R.Version()[c("major","minor")], collapse = ".")` [@R-base]. Three different approaches were applied to compare findings across the two item rating procedures. First, simple linear regression was applied to  all "Kuncel & Tellegen" functions (as explained below), extracting slope coefficients, with Edwards' ratings being correlated with these slope coefficients across items. Secondly, 300 $2^{nd}$ degree hierarchical *polynomial* regressions were applied to each function in an attempt to capture empirical "U" or "inverted-U" functional forms, and the number of items characterized by the quadratic regression term (incrementally "above and beyond" the linear) were noted. Lastly, visual perceptions of the functions were categorized, and the above noted associations were revisited within *differently categorized functional forms*.^[NOTE. Old but maybe revisit: used these groupings to help inform ranges of Edwards values along which nonlinear item functions tend to be more prominent (e.g., how many "inverted U-shaped" functions were noted in items characterized by Edwards' system  as *extremely undesirable*, *undesirable*, *average*, *desirable*, and *extremely desirable*).]

All three approaches focused on the *functional form* of "Kuncel & Tellegen" ratings, and relied upon either regression analyses to provide an empirical estimate of the function form and/or judge categorizations to provide a subjective interpretation of the function form. These functions all reflect progression across Kuncel & Tellegen frames of reference (ranging from someone who is "Extremely High in the characteristic (top 1%)" to someone who is "Extremely Low in the characteristic (below 1%)", again see Figure \@ref(fig:Figure1) for exposition). The "height" of the function at each of five rated frames of reference is determined by the *average desirability rating* at each of these points of normative evaluation.

### Approach #1: Functional Slope (Empirical)

One reasonable manifestation of similarity across the two procedures would be stronger incidences of functional linearity with extremely desirable and undesirable items (and correspondingly weaker linear associations with moderate items). For the first investigative approach, we therefore probed for associations between "Edwards"' item ratings and *regression slope* of "Kuncel & Tellegen" function. For added visual exposition, items were also categorized within arrays of values such that "Kuncel & Tellegen" functional forms could be observed within meaningful ranges of Edwardian values (e.g., moderately undesirable, extremely desirable, etc.). Figure \@ref(fig:Figure2) presents 25 Kuncel & Tellegen functional forms randomly sampled from within each of 5 different Edwardian arrays[^yabut], with the figure *rows* reflecting the array strata.  Note that the functions (even if somewhat non-monotonic - see, for example, "Seldom Daydream" in Figure \@ref(fig:Figure2)) tend to exhibit "steeper slope" with Edwards' highly desirable or undesirable items, and are "flatter" with Edwards' moderate items.

We first fit 300 individual regressions retaining the five different rated trait locations as a predictor (e.g., @kuncel_conceptual_2009's "bottom 1%", "bottom 30%", "Average", "top 30%", and "top 1%" - these were treated as representing an equal-interval numerical continuum [values of 1, 2, 3, 4, and 5])  and averaged (Edwards) response desirability rating as the criterion. Within each of the 300 individual regressions, the expectation was that slope *magnitude* and *valence* would parallel the classic Edwards ratings of the same items. For example, the expectation was that an item such as "Believe that others have good intentions" would realize a highly desirable Edwards rating as well as a high magnitude, negatively valenced slope estimate across sequential Kuncel & Tellegen categories. "Enjoy wild flights of fancy" would exhibit a moderate Edwards rating and flat slope, and "Get irritated easily" would return an undesirable Edwards rating as well as a moderately positive slope. Across all 300 items, the relationship between Edwards rating and Kuncel & Tellegen functional slope was indeed revealed to be strong (*r* = -.76, $R^2 = .58$, $F_{(1,298)} = 412.26$, *p* < .001), suggesting a non-trivial association between procedures. 

This first set of explorations merely confirms a general pattern such that more extreme "Edwards" items tend to have steeper "Kuncel & Tellegen" functions, but is not indicative of functional form (e.g., this exploration is not directly reflective of, for example, nonmonotonicity).  

[^yabut]: Note that these plotted functions progress along only 5 actual x-axis values. The "connected dots" do not imply continuous x-axis values but rather represent the focal @kuncel_conceptual_2009 functional patterns.

```{r getQualtrics, echo=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)

#data with row 2 of qualtrics as header 
data <- read_csv("form1.csv", col_names = TRUE, skip = 1)
data <- data %>% slice(3:n())

#data with row 1 of qualtrics as header
data1 <- read_csv("form1.csv", col_names = TRUE)
data1 <- data1 %>% slice(3:n())


#drop columns
data<- data[-c(1:17)]
data1<- data1[-c(1:17)]


#rename columns
colnames(data)[5]<- "i1.t1"
colnames(data)[10]<- "i1.t30"
colnames(data)[15]<- "i1.av"
colnames(data)[20]<- "i1.b30"
colnames(data)[25]<- "i1.b1"
colnames(data)[30]<- "i2.t1"
colnames(data)[35]<- "i2.t30"
colnames(data)[40]<- "i2.av"
colnames(data)[45]<- "i2.b30"
colnames(data)[50]<- "i2.b1"
colnames(data)[55]<- "i3.t1"
colnames(data)[60]<- "i3.t30"
colnames(data)[65]<- "i3.av"
colnames(data)[70]<- "i3.b30"
colnames(data)[75]<- "i3.b1"
colnames(data)[80]<- "i4.t1"
colnames(data)[85]<- "i4.t30"
colnames(data)[90]<- "i4..av"
colnames(data)[95]<- "i4.b30"
colnames(data)[100]<- "i4.b1"
colnames(data)[105]<- "i5.t1"
colnames(data)[110]<- "i5.t30"
colnames(data)[115]<- "i5.av"
colnames(data)[120]<- "i5.b30"
colnames(data)[125]<- "i5.b1"
colnames(data)[130]<- "i6.t1"
colnames(data)[135]<- "i6.t30"
colnames(data)[140]<- "i6.av"
colnames(data)[145]<- "i6.b30"
colnames(data)[150]<- "i6.b1"
colnames(data)[155]<- "i7.t1"
colnames(data)[160]<- "i7.t30"
colnames(data)[165]<- "i7.av"
colnames(data)[170]<- "i7.b30"
colnames(data)[175]<- "i7.b1"
colnames(data)[180]<- "i8.t1"
colnames(data)[185]<- "i8.t30"
colnames(data)[190]<- "i8.av"
colnames(data)[195]<- "i8.b30"
colnames(data)[200]<- "i8.b1"
colnames(data)[205]<- "i9.t1"
colnames(data)[210]<- "i9.t30"
colnames(data)[215]<- "i9.av"
colnames(data)[220]<- "i9.b30"
colnames(data)[225]<- "i9.b1"
colnames(data)[230]<- "i10.t1"
colnames(data)[235]<- "i10.t30"
colnames(data)[240]<- "i10.av"
colnames(data)[245]<- "i10.b30"
colnames(data)[250]<- "i10.b1"
colnames(data)[255]<- "i11.t1"
colnames(data)[260]<- "i11.t30"
colnames(data)[265]<- "i11.av"
colnames(data)[270]<- "i11.b30"
colnames(data)[275]<- "i11.b1"
colnames(data)[280]<- "i12.t1"
colnames(data)[285]<- "i12.t30"
colnames(data)[290]<- "i12.av"
colnames(data)[295]<- "i12.b30"
colnames(data)[300]<- "i12.b1"
colnames(data)[305]<- "i13.t1"
colnames(data)[310]<- "i13.t30"
colnames(data)[315]<- "i13.av"
colnames(data)[320]<- "i13.b30"
colnames(data)[325]<- "i13.b1"
colnames(data)[330]<- "i14.t1"
colnames(data)[335]<- "i14.t30"
colnames(data)[340]<- "i14.av"
colnames(data)[345]<- "i14.b30"
colnames(data)[350]<- "i14.b1"
colnames(data)[355]<- "i15.t1"
colnames(data)[360]<- "i15.t30"
colnames(data)[365]<- "i15.av"
colnames(data)[370]<- "i15.b30"
colnames(data)[375]<- "i15.b1"
colnames(data)[380]<- "i16.t1"
colnames(data)[385]<- "i16.t30"
colnames(data)[390]<- "i16.av"
colnames(data)[395]<- "i16.b30"
colnames(data)[400]<- "i16.b1"
colnames(data)[405]<- "i17.t1"
colnames(data)[410]<- "i17.t30"
colnames(data)[415]<- "i17.av"
colnames(data)[420]<- "i17.b30"
colnames(data)[425]<- "i17.b1"
colnames(data)[430]<- "i18.t1"
colnames(data)[435]<- "i18.t30"
colnames(data)[440]<- "i18.av"
colnames(data)[445]<- "i18.b30"
colnames(data)[450]<- "i18.b1"
colnames(data)[455]<- "i19.t1"
colnames(data)[460]<- "i19.t30"
colnames(data)[465]<- "i19.av"
colnames(data)[470]<- "i19.b30"
colnames(data)[475]<- "i19.b1"
colnames(data)[480]<- "i20.t1"
colnames(data)[485]<- "i20.t30"
colnames(data)[490]<- "i20.av"
colnames(data)[495]<- "i20.b30"
colnames(data)[500]<- "i20.b1"
colnames(data)[505]<- "i21.t1"
colnames(data)[510]<- "i21.t30"
colnames(data)[515]<- "i21.av"
colnames(data)[520]<- "i21.b30"
colnames(data)[525]<- "i21.b1"
colnames(data)[530]<- "i22.t1"
colnames(data)[535]<- "i22.t30"
colnames(data)[540]<- "i22.av"
colnames(data)[545]<- "i22.b30"
colnames(data)[550]<- "i22.b1"
colnames(data)[555]<- "i23.t1"
colnames(data)[560]<- "i23.t30"
colnames(data)[565]<- "i23.av"
colnames(data)[570]<- "i23.b30"
colnames(data)[575]<- "i23.b1"
colnames(data)[580]<- "i24.t1"
colnames(data)[585]<- "i24.t30"
colnames(data)[590]<- "i24.av"
colnames(data)[595]<- "i24.b30"
colnames(data)[600]<- "i24.b1"
colnames(data)[605]<- "i25.t1"
colnames(data)[610]<- "i25.t30"
colnames(data)[615]<- "i25.av"
colnames(data)[620]<- "i25.b30"
colnames(data)[625]<- "i25.b1"
colnames(data)[630]<- "i26.t1"
colnames(data)[635]<- "i26.t30"
colnames(data)[640]<- "i26.av"
colnames(data)[645]<- "i26.b30"
colnames(data)[650]<- "i26.b1"
colnames(data)[655]<- "i27.t1"
colnames(data)[660]<- "i27.t30"
colnames(data)[665]<- "i27.av"
colnames(data)[670]<- "i27.b30"
colnames(data)[675]<- "i27.b1"
colnames(data)[680]<- "i28.t1"
colnames(data)[685]<- "i28.t30"
colnames(data)[690]<- "i28.av"
colnames(data)[695]<- "i28.b30"
colnames(data)[700]<- "i28.b1"
colnames(data)[705]<- "i29.t1"
colnames(data)[710]<- "i29.t30"
colnames(data)[715]<- "i29.av"
colnames(data)[720]<- "i29.b30"
colnames(data)[725]<- "i29.b1"
colnames(data)[730]<- "i30.t1"
colnames(data)[735]<- "i30.t30"
colnames(data)[740]<- "i30.av"
colnames(data)[745]<- "i30.b30"
colnames(data)[750]<- "i30.b1"
colnames(data)[755]<- "i31.t1"
colnames(data)[760]<- "i31.t30"
colnames(data)[765]<- "i31.av"
colnames(data)[770]<- "i31.b30"
colnames(data)[775]<- "i31.b1"
colnames(data)[780]<- "i32.t1"
colnames(data)[785]<- "i32.t30"
colnames(data)[790]<- "i32.av"
colnames(data)[795]<- "i32.b30"
colnames(data)[800]<- "i32.b1"
colnames(data)[805]<- "i33.t1"
colnames(data)[810]<- "i33.t30"
colnames(data)[815]<- "i33.av"
colnames(data)[820]<- "i33.b30"
colnames(data)[825]<- "i33.b1"
colnames(data)[830]<- "i34.t1"
colnames(data)[835]<- "i34.t30"
colnames(data)[840]<- "i34.av"
colnames(data)[845]<- "i34.b30"
colnames(data)[850]<- "i34.b1"
colnames(data)[855]<- "i35.t1"
colnames(data)[860]<- "i35.t30"
colnames(data)[865]<- "i35.av"
colnames(data)[870]<- "i35.b30"
colnames(data)[875]<- "i35.b1"
colnames(data)[880]<- "i36.t1"
colnames(data)[885]<- "i36.t30"
colnames(data)[890]<- "i36.av"
colnames(data)[895]<- "i36.b30"
colnames(data)[900]<- "i36.b1"
colnames(data)[905]<- "i37.t1"
colnames(data)[910]<- "i37.t30"
colnames(data)[915]<- "i37.av"
colnames(data)[920]<- "i37.b30"
colnames(data)[925]<- "i37.b1"
colnames(data)[930]<- "i38.t1"
colnames(data)[935]<- "i38.t30"
colnames(data)[940]<- "i38.av"
colnames(data)[945]<- "i38.b30"
colnames(data)[950]<- "i38.b1"
colnames(data)[955]<- "i39.t1"
colnames(data)[960]<- "i39.t30"
colnames(data)[965]<- "i39.av"
colnames(data)[970]<- "i39.b30"
colnames(data)[975]<- "i39.b1"
colnames(data)[980]<- "i40.t1"
colnames(data)[985]<- "i40.t30"
colnames(data)[990]<- "i40.av"
colnames(data)[995]<- "i40.b30"
colnames(data)[1000]<- "i40.b1"
colnames(data)[1005]<- "i41.t1"
colnames(data)[1010]<- "i41.t30"
colnames(data)[1015]<- "i41.av"
colnames(data)[1020]<- "i41.b30"
colnames(data)[1025]<- "i41.b1"
colnames(data)[1030]<- "i42.t1"
colnames(data)[1035]<- "i42.t30"
colnames(data)[1040]<- "i42.av"
colnames(data)[1045]<- "i42.b30"
colnames(data)[1050]<- "i42.b1"
colnames(data)[1055]<- "i43.t1"
colnames(data)[1060]<- "i43.t30"
colnames(data)[1065]<- "i43.av"
colnames(data)[1070]<- "i43.b30"
colnames(data)[1075]<- "i43.b1"
colnames(data)[1080]<- "i44.t1"
colnames(data)[1085]<- "i44.t30"
colnames(data)[1090]<- "i44.av"
colnames(data)[1095]<- "i44.b30"
colnames(data)[1100]<- "i44.b1"
colnames(data)[1105]<- "i45.t1"
colnames(data)[1110]<- "i45.t30"
colnames(data)[1115]<- "i45.av"
colnames(data)[1120]<- "i45.b30"
colnames(data)[1125]<- "i45.b1"
colnames(data)[1130]<- "i46.t1"
colnames(data)[1135]<- "i46.t30"
colnames(data)[1140]<- "i46.av"
colnames(data)[1145]<- "i46.b30"
colnames(data)[1150]<- "i46.b1"
colnames(data)[1155]<- "i47.t1"
colnames(data)[1160]<- "i47.t30"
colnames(data)[1165]<- "i47.av"
colnames(data)[1170]<- "i47.b30"
colnames(data)[1175]<- "i47.b1"
colnames(data)[1180]<- "i48.t1"
colnames(data)[1185]<- "i48.t30"
colnames(data)[1190]<- "i48.av"
colnames(data)[1195]<- "i48.b30"
colnames(data)[1200]<- "i48.b1"
colnames(data)[1205]<- "i49.t1"
colnames(data)[1210]<- "i49.t30"
colnames(data)[1215]<- "i49.av"
colnames(data)[1220]<- "i49.b30"
colnames(data)[1225]<- "i49.b1"
colnames(data)[1230]<- "i50.t1"
colnames(data)[1235]<- "i50.t30"
colnames(data)[1240]<- "i50.av"
colnames(data)[1245]<- "i50.b30"
colnames(data)[1250]<- "i50.b1"
colnames(data)[1255]<- "i51.t1"
colnames(data)[1260]<- "i51.t30"
colnames(data)[1265]<- "i51.av"
colnames(data)[1270]<- "i51.b30"
colnames(data)[1275]<- "i51.b1"
colnames(data)[1280]<- "i52.t1"
colnames(data)[1285]<- "i52.t30"
colnames(data)[1290]<- "i52.av"
colnames(data)[1295]<- "i52.b30"
colnames(data)[1300]<- "i52.b1"
colnames(data)[1305]<- "i53.t1"
colnames(data)[1310]<- "i53.t30"
colnames(data)[1315]<- "i53.av"
colnames(data)[1320]<- "i53.b30"
colnames(data)[1325]<- "i53.b1"
colnames(data)[1330]<- "i54.t1"
colnames(data)[1335]<- "i54.t30"
colnames(data)[1340]<- "i54.av"
colnames(data)[1345]<- "i54.b30"
colnames(data)[1350]<- "i54.b1"
colnames(data)[1355]<- "i55.t1"
colnames(data)[1360]<- "i55.t30"
colnames(data)[1365]<- "i55.av"
colnames(data)[1370]<- "i55.b30"
colnames(data)[1375]<- "i55.b1"
colnames(data)[1380]<- "i56.t1"
colnames(data)[1385]<- "i56.t30"
colnames(data)[1390]<- "i56.av"
colnames(data)[1395]<- "i56.b30"
colnames(data)[1400]<- "i56.b1"
colnames(data)[1405]<- "i57.t1"
colnames(data)[1410]<- "i57.t30"
colnames(data)[1415]<- "i57.av"
colnames(data)[1420]<- "i57.b30"
colnames(data)[1425]<- "i57.b1"
colnames(data)[1430]<- "i58.t1"
colnames(data)[1435]<- "i58.t30"
colnames(data)[1440]<- "i58.av"
colnames(data)[1445]<- "i58.b30"
colnames(data)[1450]<- "i58.b1"
colnames(data)[1455]<- "i59.t1"
colnames(data)[1460]<- "i59.t30"
colnames(data)[1465]<- "i59.av"
colnames(data)[1470]<- "i59.b30"
colnames(data)[1475]<- "i59.b1"
colnames(data)[1480]<- "i60.t1"
colnames(data)[1485]<- "i60.t30"
colnames(data)[1490]<- "i60.av"
colnames(data)[1495]<- "i60.b30"
colnames(data)[1500]<- "i60.b1"

#renaming response latency columns
colnames(data)[1]<- "i1.t1_lat"
colnames(data)[6]<- "i1.t30_lat"
colnames(data)[11]<- "i1.av_lat"
colnames(data)[16]<- "i1.b30_lat"
colnames(data)[21]<- "i1.b1_lat"
colnames(data)[26]<- "i2.t1_lat"
colnames(data)[31]<- "i2.t30_lat"
colnames(data)[36]<- "i2.av_lat"
colnames(data)[41]<- "i2.b30_lat"
colnames(data)[46]<- "i2.b1_lat"
colnames(data)[51]<- "i3.t1_lat"
colnames(data)[56]<- "i3.t30_lat"
colnames(data)[61]<- "i3.av_lat"
colnames(data)[66]<- "i3.b30_lat"
colnames(data)[71]<- "i3.b1_lat"
colnames(data)[76]<- "i4.t1_lat"
colnames(data)[81]<- "i4.t30_lat"
colnames(data)[86]<- "i4..av_lat"
colnames(data)[91]<- "i4.b30_lat"
colnames(data)[96]<- "i4.b1_lat"
colnames(data)[101]<- "i5.t1_lat"
colnames(data)[106]<- "i5.t30_lat"
colnames(data)[111]<- "i5.av_lat"
colnames(data)[116]<- "i5.b30_lat"
colnames(data)[121]<- "i5.b1_lat"
colnames(data)[126]<- "i6.t1_lat"
colnames(data)[131]<- "i6.t30_lat"
colnames(data)[136]<- "i6.av_lat"
colnames(data)[141]<- "i6.b30_lat"
colnames(data)[146]<- "i6.b1_lat"
colnames(data)[151]<- "i7.t1_lat"
colnames(data)[156]<- "i7.t30_lat"
colnames(data)[161]<- "i7.av_lat"
colnames(data)[166]<- "i7.b30_lat"
colnames(data)[171]<- "i7.b1_lat"
colnames(data)[176]<- "i8.t1_lat"
colnames(data)[181]<- "i8.t30_lat"
colnames(data)[186]<- "i8.av_lat"
colnames(data)[191]<- "i8.b30_lat"
colnames(data)[196]<- "i8.b1_lat"
colnames(data)[201]<- "i9.t1_lat"
colnames(data)[206]<- "i9.t30_lat"
colnames(data)[211]<- "i9.av_lat"
colnames(data)[216]<- "i9.b30_lat"
colnames(data)[221]<- "i9.b1_lat"
colnames(data)[226]<- "i10.t1_lat"
colnames(data)[231]<- "i10.t30_lat"
colnames(data)[236]<- "i10.av_lat"
colnames(data)[241]<- "i10.b30_lat"
colnames(data)[246]<- "i10.b1_lat"
colnames(data)[251]<- "i11.t1_lat"
colnames(data)[256]<- "i11.t30_lat"
colnames(data)[261]<- "i11.av_lat"
colnames(data)[266]<- "i11.b30_lat"
colnames(data)[271]<- "i11.b1_lat"
colnames(data)[276]<- "i12.t1_lat"
colnames(data)[281]<- "i12.t30_lat"
colnames(data)[286]<- "i12.av_lat"
colnames(data)[291]<- "i12.b30_lat"
colnames(data)[296]<- "i12.b1_lat"
colnames(data)[301]<- "i13.t1_lat"
colnames(data)[306]<- "i13.t30_lat"
colnames(data)[311]<- "i13.av_lat"
colnames(data)[316]<- "i13.b30_lat"
colnames(data)[321]<- "i13.b1_lat"
colnames(data)[326]<- "i14.t1_lat"
colnames(data)[331]<- "i14.t30_lat"
colnames(data)[336]<- "i14.av_lat"
colnames(data)[341]<- "i14.b30_lat"
colnames(data)[346]<- "i14.b1_lat"
colnames(data)[351]<- "i15.t1_lat"
colnames(data)[356]<- "i15.t30_lat"
colnames(data)[361]<- "i15.av_lat"
colnames(data)[366]<- "i15.b30_lat"
colnames(data)[371]<- "i15.b1_lat"
colnames(data)[376]<- "i16.t1_lat"
colnames(data)[381]<- "i16.t30_lat"
colnames(data)[386]<- "i16.av_lat"
colnames(data)[391]<- "i16.b30_lat"
colnames(data)[396]<- "i16.b1_lat"
colnames(data)[401]<- "i17.t1_lat"
colnames(data)[406]<- "i17.t30_lat"
colnames(data)[411]<- "i17.av_lat"
colnames(data)[416]<- "i17.b30_lat"
colnames(data)[421]<- "i17.b1_lat"
colnames(data)[426]<- "i18.t1_lat"
colnames(data)[431]<- "i18.t30_lat"
colnames(data)[436]<- "i18.av_lat"
colnames(data)[441]<- "i18.b30_lat"
colnames(data)[446]<- "i18.b1_lat"
colnames(data)[451]<- "i19.t1_lat"
colnames(data)[456]<- "i19.t30_lat"
colnames(data)[461]<- "i19.av_lat"
colnames(data)[466]<- "i19.b30_lat"
colnames(data)[471]<- "i19.b1_lat"
colnames(data)[476]<- "i20.t1_lat"
colnames(data)[481]<- "i20.t30_lat"
colnames(data)[486]<- "i20.av_lat"
colnames(data)[491]<- "i20.b30_lat"
colnames(data)[496]<- "i20.b1_lat"
colnames(data)[501]<- "i21.t1_lat"
colnames(data)[506]<- "i21.t30_lat"
colnames(data)[511]<- "i21.av_lat"
colnames(data)[516]<- "i21.b30_lat"
colnames(data)[521]<- "i21.b1_lat"
colnames(data)[526]<- "i22.t1_lat"
colnames(data)[531]<- "i22.t30_lat"
colnames(data)[536]<- "i22.av_lat"
colnames(data)[541]<- "i22.b30_lat"
colnames(data)[546]<- "i22.b1_lat"
colnames(data)[551]<- "i23.t1_lat"
colnames(data)[556]<- "i23.t30_lat"
colnames(data)[561]<- "i23.av_lat"
colnames(data)[566]<- "i23.b30_lat"
colnames(data)[571]<- "i23.b1_lat"
colnames(data)[576]<- "i24.t1_lat"
colnames(data)[581]<- "i24.t30_lat"
colnames(data)[586]<- "i24.av_lat"
colnames(data)[591]<- "i24.b30_lat"
colnames(data)[596]<- "i24.b1_lat"
colnames(data)[601]<- "i25.t1_lat"
colnames(data)[606]<- "i25.t30_lat"
colnames(data)[611]<- "i25.av_lat"
colnames(data)[616]<- "i25.b30_lat"
colnames(data)[621]<- "i25.b1_lat"
colnames(data)[626]<- "i26.t1_lat"
colnames(data)[631]<- "i26.t30_lat"
colnames(data)[636]<- "i26.av_lat"
colnames(data)[641]<- "i26.b30_lat"
colnames(data)[646]<- "i26.b1_lat"
colnames(data)[651]<- "i27.t1_lat"
colnames(data)[656]<- "i27.t30_lat"
colnames(data)[661]<- "i27.av_lat"
colnames(data)[666]<- "i27.b30_lat"
colnames(data)[671]<- "i27.b1_lat"
colnames(data)[676]<- "i28.t1_lat"
colnames(data)[681]<- "i28.t30_lat"
colnames(data)[686]<- "i28.av_lat"
colnames(data)[691]<- "i28.b30_lat"
colnames(data)[696]<- "i28.b1_lat"
colnames(data)[701]<- "i29.t1_lat"
colnames(data)[706]<- "i29.t30_lat"
colnames(data)[711]<- "i29.av_lat"
colnames(data)[716]<- "i29.b30_lat"
colnames(data)[721]<- "i29.b1_lat"
colnames(data)[726]<- "i30.t1_lat"
colnames(data)[731]<- "i30.t30_lat"
colnames(data)[736]<- "i30.av_lat"
colnames(data)[741]<- "i30.b30_lat"
colnames(data)[746]<- "i30.b1_lat"
colnames(data)[751]<- "i31.t1_lat"
colnames(data)[756]<- "i31.t30_lat"
colnames(data)[761]<- "i31.av_lat"
colnames(data)[766]<- "i31.b30_lat"
colnames(data)[771]<- "i31.b1_lat"
colnames(data)[776]<- "i32.t1_lat"
colnames(data)[781]<- "i32.t30_lat"
colnames(data)[786]<- "i32.av_lat"
colnames(data)[791]<- "i32.b30_lat"
colnames(data)[796]<- "i32.b1_lat"
colnames(data)[801]<- "i33.t1_lat"
colnames(data)[806]<- "i33.t30_lat"
colnames(data)[811]<- "i33.av_lat"
colnames(data)[816]<- "i33.b30_lat"
colnames(data)[821]<- "i33.b1_lat"
colnames(data)[826]<- "i34.t1_lat"
colnames(data)[831]<- "i34.t30_lat"
colnames(data)[836]<- "i34.av_lat"
colnames(data)[841]<- "i34.b30_lat"
colnames(data)[846]<- "i34.b1_lat"
colnames(data)[851]<- "i35.t1_lat"
colnames(data)[856]<- "i35.t30_lat"
colnames(data)[861]<- "i35.av_lat"
colnames(data)[866]<- "i35.b30_lat"
colnames(data)[871]<- "i35.b1_lat"
colnames(data)[876]<- "i36.t1_lat"
colnames(data)[881]<- "i36.t30_lat"
colnames(data)[886]<- "i36.av_lat"
colnames(data)[891]<- "i36.b30_lat"
colnames(data)[896]<- "i36.b1_lat"
colnames(data)[901]<- "i37.t1_lat"
colnames(data)[906]<- "i37.t30_lat"
colnames(data)[911]<- "i37.av_lat"
colnames(data)[916]<- "i37.b30_lat"
colnames(data)[921]<- "i37.b1_lat"
colnames(data)[926]<- "i38.t1_lat"
colnames(data)[931]<- "i38.t30_lat"
colnames(data)[936]<- "i38.av_lat"
colnames(data)[941]<- "i38.b30_lat"
colnames(data)[946]<- "i38.b1_lat"
colnames(data)[951]<- "i39.t1_lat"
colnames(data)[956]<- "i39.t30_lat"
colnames(data)[961]<- "i39.av_lat"
colnames(data)[966]<- "i39.b30_lat"
colnames(data)[971]<- "i39.b1_lat"
colnames(data)[976]<- "i40.t1_lat"
colnames(data)[981]<- "i40.t30_lat"
colnames(data)[986]<- "i40.av_lat"
colnames(data)[991]<- "i40.b30_lat"
colnames(data)[996]<- "i40.b1_lat"
colnames(data)[1001]<- "i41.t1_lat"
colnames(data)[1006]<- "i41.t30_lat"
colnames(data)[1011]<- "i41.av_lat"
colnames(data)[1016]<- "i41.b30_lat"
colnames(data)[1021]<- "i41.b1_lat"
colnames(data)[1026]<- "i42.t1_lat"
colnames(data)[1031]<- "i42.t30_lat"
colnames(data)[1036]<- "i42.av_lat"
colnames(data)[1041]<- "i42.b30_lat"
colnames(data)[1046]<- "i42.b1_lat"
colnames(data)[1051]<- "i43.t1_lat"
colnames(data)[1056]<- "i43.t30_lat"
colnames(data)[1061]<- "i43.av_lat"
colnames(data)[1066]<- "i43.b30_lat"
colnames(data)[1071]<- "i43.b1_lat"
colnames(data)[1076]<- "i44.t1_lat"
colnames(data)[1081]<- "i44.t30_lat"
colnames(data)[1086]<- "i44.av_lat"
colnames(data)[1091]<- "i44.b30_lat"
colnames(data)[1096]<- "i44.b1_lat"
colnames(data)[1101]<- "i45.t1_lat"
colnames(data)[1106]<- "i45.t30_lat"
colnames(data)[1111]<- "i45.av_lat"
colnames(data)[1116]<- "i45.b30_lat"
colnames(data)[1121]<- "i45.b1_lat"
colnames(data)[1126]<- "i46.t1_lat"
colnames(data)[1131]<- "i46.t30_lat"
colnames(data)[1136]<- "i46.av_lat"
colnames(data)[1141]<- "i46.b30_lat"
colnames(data)[1146]<- "i46.b1_lat"
colnames(data)[1151]<- "i47.t1_lat"
colnames(data)[1156]<- "i47.t30_lat"
colnames(data)[1161]<- "i47.av_lat"
colnames(data)[1166]<- "i47.b30_lat"
colnames(data)[1171]<- "i47.b1_lat"
colnames(data)[1176]<- "i48.t1_lat"
colnames(data)[1181]<- "i48.t30_lat"
colnames(data)[1186]<- "i48.av_lat"
colnames(data)[1191]<- "i48.b30_lat"
colnames(data)[1196]<- "i48.b1_lat"
colnames(data)[1201]<- "i49.t1_lat"
colnames(data)[1206]<- "i49.t30_lat"
colnames(data)[1211]<- "i49.av_lat"
colnames(data)[1216]<- "i49.b30_lat"
colnames(data)[1221]<- "i49.b1_lat"
colnames(data)[1226]<- "i50.t1_lat"
colnames(data)[1231]<- "i50.t30_lat"
colnames(data)[1236]<- "i50.av_lat"
colnames(data)[1241]<- "i50.b30_lat"
colnames(data)[1246]<- "i50.b1_lat"
colnames(data)[1251]<- "i51.t1_lat"
colnames(data)[1256]<- "i51.t30_lat"
colnames(data)[1261]<- "i51.av_lat"
colnames(data)[1266]<- "i51.b30_lat"
colnames(data)[1271]<- "i51.b1_lat"
colnames(data)[1276]<- "i52.t1_lat"
colnames(data)[1281]<- "i52.t30_lat"
colnames(data)[1286]<- "i52.av_lat"
colnames(data)[1291]<- "i52.b30_lat"
colnames(data)[1296]<- "i52.b1_lat"
colnames(data)[1301]<- "i53.t1_lat"
colnames(data)[1306]<- "i53.t30_lat"
colnames(data)[1311]<- "i53.av_lat"
colnames(data)[1316]<- "i53.b30_lat"
colnames(data)[1321]<- "i53.b1_lat"
colnames(data)[1326]<- "i54.t1_lat"
colnames(data)[1331]<- "i54.t30_lat"
colnames(data)[1336]<- "i54.av_lat"
colnames(data)[1341]<- "i54.b30_lat"
colnames(data)[1346]<- "i54.b1_lat"
colnames(data)[1351]<- "i55.t1_lat"
colnames(data)[1356]<- "i55.t30_lat"
colnames(data)[1361]<- "i55.av_lat"
colnames(data)[1366]<- "i55.b30_lat"
colnames(data)[1371]<- "i55.b1_lat"
colnames(data)[1376]<- "i56.t1_lat"
colnames(data)[1381]<- "i56.t30_lat"
colnames(data)[1386]<- "i56.av_lat"
colnames(data)[1391]<- "i56.b30_lat"
colnames(data)[1396]<- "i56.b1_lat"
colnames(data)[1401]<- "i57.t1_lat"
colnames(data)[1406]<- "i57.t30_lat"
colnames(data)[1411]<- "i57.av_lat"
colnames(data)[1416]<- "i57.b30_lat"
colnames(data)[1421]<- "i57.b1_lat"
colnames(data)[1426]<- "i58.t1_lat"
colnames(data)[1431]<- "i58.t30_lat"
colnames(data)[1436]<- "i58.av_lat"
colnames(data)[1441]<- "i58.b30_lat"
colnames(data)[1446]<- "i58.b1_lat"
colnames(data)[1451]<- "i59.t1_lat"
colnames(data)[1456]<- "i59.t30_lat"
colnames(data)[1461]<- "i59.av_lat"
colnames(data)[1466]<- "i59.b30_lat"
colnames(data)[1471]<- "i59.b1_lat"
colnames(data)[1476]<- "i60.t1_lat"
colnames(data)[1481]<- "i60.t30_lat"
colnames(data)[1486]<- "i60.av_lat"
colnames(data)[1491]<- "i60.b30_lat"
colnames(data)[1496]<- "i60.b1_lat"



#separating data into new dataframe with only the response latencies
library(psych)
#latencies
n<- data[seq(1,length(data),5)]
#responses
m<- data[seq(5,length(data),5)]
#getting std devs of the columns of new dataframe

n1 = as.data.frame(sapply(n,as.numeric))

sdlat<- sapply(n1,sd, na.rm=TRUE)

library(plyr)
sd_lat<- colwise(sd)(n1)
sd_res <- colwise(sd)(m) 

##sds of scores
m1= as.data.frame(sapply(m,as.numeric))
sdscore<-  sapply(m1,sd, na.rm=TRUE)


##TO DO:
  #freq distribution of response latencies
  #one variable with all latencies
  #then get a freq distribution of that
library(descr)


latr<- round(sdlat,1)
freq(latr)
hist(latr)

library(epiDisplay)
tab1(latr)


latscorer<- round(sdscore,1)
freq(latscorer)
```


```{r Figure2, echo=FALSE, fig.cap="Kuncel & Tellegen (2009) functional patterns across Edwards (1953) scale value strata.", fig.height=8, fig.width=11}


kun_sort <- read.csv("Kuncel Tellegen.csv")

form1 <- kun_sort[kun_sort$form == 1, ]
form2 <- kun_sort[kun_sort$form == 2, ]
form3 <- kun_sort[kun_sort$form == 3, ]
form4 <- kun_sort[kun_sort$form == 4, ]
form5 <- kun_sort[kun_sort$form == 5, ]
form6 <- kun_sort[kun_sort$form == 6, ]
form7 <- kun_sort[kun_sort$form == 7, ]
form8 <- kun_sort[kun_sort$form == 8, ]
form9 <- kun_sort[kun_sort$form == 9, ]
form10 <- kun_sort[kun_sort$form == 10, ]


form2r <- form2[,c(1:2, 298:302, 293:297, 288:292, 283:287, 278:282, 273:277, 268:272, 263:267,
                   258:262, 253:257, 248:252, 243:247, 238:242, 233:237, 228:232, 223:227,
                   218:222, 213:217, 208:212, 203:207, 198:202, 193:197, 188:192, 183:187,
                   178:182, 173:177, 168:172, 163:167, 158:162, 153:157, 148:152, 143:147,
                   138:142, 133:137, 128:132, 123:127, 118:122, 113:117, 108:112, 103:107,
                   98:102, 93:97, 88:92, 83:87, 78:82, 73:77, 68:72, 63:67, 58:62, 53:57,
                   48:52, 43:47, 38:42, 33:37, 28:32, 23:27, 18:22, 13:17, 08:12, 03:07, 303)]

form4r <- form4[,c(1:2, 298:302, 293:297, 288:292, 283:287, 278:282, 273:277, 268:272, 263:267,
                   258:262, 253:257, 248:252, 243:247, 238:242, 233:237, 228:232, 223:227,
                   218:222, 213:217, 208:212, 203:207, 198:202, 193:197, 188:192, 183:187,
                   178:182, 173:177, 168:172, 163:167, 158:162, 153:157, 148:152, 143:147,
                   138:142, 133:137, 128:132, 123:127, 118:122, 113:117, 108:112, 103:107,
                   98:102, 93:97, 88:92, 83:87, 78:82, 73:77, 68:72, 63:67, 58:62, 53:57,
                   48:52, 43:47, 38:42, 33:37, 28:32, 23:27, 18:22, 13:17, 08:12, 03:07, 303)]

form6r <- form6[,c(1:2, 298:302, 293:297, 288:292, 283:287, 278:282, 273:277, 268:272, 263:267,
                   258:262, 253:257, 248:252, 243:247, 238:242, 233:237, 228:232, 223:227,
                   218:222, 213:217, 208:212, 203:207, 198:202, 193:197, 188:192, 183:187,
                   178:182, 173:177, 168:172, 163:167, 158:162, 153:157, 148:152, 143:147,
                   138:142, 133:137, 128:132, 123:127, 118:122, 113:117, 108:112, 103:107,
                   98:102, 93:97, 88:92, 83:87, 78:82, 73:77, 68:72, 63:67, 58:62, 53:57,
                   48:52, 43:47, 38:42, 33:37, 28:32, 23:27, 18:22, 13:17, 08:12, 03:07, 303)]

form8r <- form8[,c(1:2, 298:302, 293:297, 288:292, 283:287, 278:282, 273:277, 268:272, 263:267,
                   258:262, 253:257, 248:252, 243:247, 238:242, 233:237, 228:232, 223:227,
                   218:222, 213:217, 208:212, 203:207, 198:202, 193:197, 188:192, 183:187,
                   178:182, 173:177, 168:172, 163:167, 158:162, 153:157, 148:152, 143:147,
                   138:142, 133:137, 128:132, 123:127, 118:122, 113:117, 108:112, 103:107,
                   98:102, 93:97, 88:92, 83:87, 78:82, 73:77, 68:72, 63:67, 58:62, 53:57,
                   48:52, 43:47, 38:42, 33:37, 28:32, 23:27, 18:22, 13:17, 08:12, 03:07, 303)]

form10r <- form10[,c(1:2, 298:302, 293:297, 288:292, 283:287, 278:282, 273:277, 268:272, 263:267,
                     258:262, 253:257, 248:252, 243:247, 238:242, 233:237, 228:232, 223:227,
                     218:222, 213:217, 208:212, 203:207, 198:202, 193:197, 188:192, 183:187,
                     178:182, 173:177, 168:172, 163:167, 158:162, 153:157, 148:152, 143:147,
                     138:142, 133:137, 128:132, 123:127, 118:122, 113:117, 108:112, 103:107,
                     98:102, 93:97, 88:92, 83:87, 78:82, 73:77, 68:72, 63:67, 58:62, 53:57,
                     48:52, 43:47, 38:42, 33:37, 28:32, 23:27, 18:22, 13:17, 08:12, 03:07, 303)]

names(form2r) <- names(form1)
names(form4r) <- names(form3)
names(form6r) <- names(form5)
names(form2r) <- names(form7)
names(form10r) <- names(form9)


##combine forms (1&2) (3&4) (5&6) (7&8) (9&10)

form12 <- rbind(form1,form2r)
form34 <- rbind(form3,form4r)
form56 <- rbind(form5,form6r)
form78 <- rbind(form7,form8r)
form910 <- rbind(form9,form10r)

plot12 <- as.data.frame(sapply(form12[3:302], mean, na.rm=TRUE))
plot34 <- as.data.frame(sapply(form34[3:302], mean, na.rm=TRUE))
plot56 <- as.data.frame(sapply(form56[3:302], mean, na.rm=TRUE))
plot78 <- as.data.frame(sapply(form78[3:302], mean, na.rm=TRUE))
plot910 <- as.data.frame(sapply(form910[3:302], mean, na.rm=TRUE))

colnames(plot12) <- "mean"
colnames(plot34) <- "mean"
colnames(plot56) <- "mean"
colnames(plot78) <- "mean"
colnames(plot910) <- "mean"

data <- as.data.frame(rbind(plot12,plot34,plot56,plot78,plot910))

#######################################################################################

data$item <- NA      ## want to create a new variable called "item" that we can select from
data$level <- NA     

level <- c("Extremely High", "Above Average", "Average", "Below Average", "Extremely Low")
data$level <- rep(level, 300)


## actually probably want item text for item values

##make these UC first word and LC other words.
items <- c("Worry about things"
           ,"Make friends easily"
           ,"Have a vivid imagination"
           ,"Trust others"
           ,"Complete tasks successfully"
           ,"Get angry easily"
           ,"Love large parties"
           ,"Believe in the importance"
           ,"Would never cheat on my taxes"
           ,"Like order"
           ,"Often feel blue"
           ,"Take Charge"
           ,"Experience my emotions intensely"
           ,"Make people feel welcome"
           ,"Try to follow the rules"
           ,"Am easily intimidated"
           ,"Am always busy"
           ,"Prefer variety to routine"
           ,"Am easy to satisfy"
           ,"Go straight for the goal"
           ,"Often eat too much"
           ,"Love excitement"
           ,"Like to solve complex problems"
           ,"Dislike being the center of attention"
           ,"Get chores done right away"
           ,"Panic easily"
           ,"Radiate Joy"
           ,"Tend to vote for liberal political candidates"
           ,"Sympathize with the homeless"
           ,"Avoid mistakes"
           ,"Fear for the worst"
           ,"Warm up quickly to others"
           ,"Enjoy wild flights of fantasy"
           ,"Believe that others have good intentions"
           ,"Excel in what I do"
           ,"Get irritated easily"
           ,"Talk to a lot of different people at parties"
           ,"Like music"
           ,"Stick to the rules"
           ,"Like to tidy up"
           ,"Dislike myself"
           ,"Try to lead others"
           ,"Feel others' emotions"
           ,"Anticipate the needs of others"
           ,"Keep my promises"
           ,"Am afraid that I will do the wrong thing"
           ,"Am always on the go"
           ,"Like to visit new places"
           ,"Can't stand confrontations"
           ,"Work hard"
           ,"Don't know why I do some of the things I do"
           ,"Seek adventure"
           ,"Love to read challenging material"
           ,"Dislike talking about myself"
           ,"Am always prepared"
           ,"Become overwhelmed by events"
           ,"Have a lot of fun"
           ,"Believe that there is no absolute right or wrong"
           ,"Feel sympathy for those who are worse off than myself"
           ,"Choose my words with care"
           ,"Am afraid of many things"
           ,"Feel comfortable around people"
           ,"Love to daydream"
           ,"Trust what people say"
           ,"Handle tasks smoothly"
           ,"Get upset easily"
           ,"Enjoy being part of a group"
           ,"See beauty in things that others might not notice"
           ,"Use flattery to get ahead"
           ,"Want everything to be 'just right'"
           
           
           
           
           
           
           
           
           
           
           ,"Am often down in the dumps"
           ,"Can talk others into doing things"
           ,"Am passionate about causes"
           ,"Love to help others"
           ,"Pay my bills on time"
           ,"Find it difficult to approach others"
           ,"Do a lot in my spare time"
           ,"Interested in many things"
           ,"Hate to seem pushy"
           ,"Turn plans into actions"
           ,"Do things I later regret"
           ,"Love action"
           ,"Have a rich vocabulary"
           ,"Consider myself an average person"
           ,"Start tasks right away"
           ,"Feel that I'm unable to deal with things"
           ,"Express childlike joy"
           ,"Believe that criminals should receive help rather than punishment"
           ,"Value cooperation over competition"
           ,"Stick to my chosen path"
           ,"Get stressed out easily"
           ,"Act comfortably with others"
           ,"Like to get lost in thought"
           ,"Believe that people are basically moral"
           ,"Am sure of my ground"
           ,"Am often in a bad mood"
           ,"Involve others in what I am doing"
           ,"Love flowers"
           ,"Use others for my own ends"
           ,"Love order and regularity"
           ,"Have a low opinion of myself"
           ,"Seek to influence others"
           ,"Enjoy examining myself and my life"
           ,"Am concerned about others"
           ,"Tell the truth"
           ,"Am afraid to draw attention to myself"
           ,"Can manage many things at the same time"
           ,"Like to begin new things"
           ,"Have a sharp tongue"
           ,"Plunge into tasks with all my heart"
           ,"Go on binges"
           ,"Enjoy being part of a loud crowd"
           ,"Can handle a lot of information"
           ,"Seldom toot my own horn"
           ,"Get to work at once"
           ,"Can't make up my mind"
           ,"Laugh my way through life"
           ,"Believe in one true religion"
           ,"Suffer from others' sorrows"
           ,"Jump into things without thinking"
           ,"Get caught up in my problems"
           ,"Cheer people up"
           ,"Induldge in my fantasies"
           ,"Believe in human goodness"
           ,"Come up with good solutions"
           ,"Lose my temper"
           ,"Love surprise parties"
           ,"Enjoy the beauty of nature"
           ,"Know how to get around the rules"
           ,"Do things according to a plan"
           ,"Have frequent mood swings"
           ,"Take control of things"
           ,"Try to understand myself"
           ,"Have a good word for everyone"
           ,"Listen to my conscience"
           ,"Only feel comfortable with friends"
           ,"React quickly"
           ,"Prefer to stik with things that I know"
           ,"Contradict others"
           ,"Do more than what's expected of me"
           ,"Love to eat"
           ,"Enjoy being reckless"
           ,"Enjoy thinking about things"
           ,"Believe that I am better than others"
           ,"Carry out my plans"
           ,"Get overwhelmed by emotions"
           ,"Love life"
           ,"Tend to vote for conservative political candidates"
           ,"Am not interested in other people's problems"
           ,"Make rash decisions"
           ,"Am not easily bothered by things"
           ,"Am hard to get to know"
           ,"Spend time reflecting on things"
           ,"Think that all will be well"
           ,"Know how to get things done"
           ,"Rarely get irritated"
           ,"Prefer to be alone"
           ,"Do not like art"
           ,"Cheat to get ahead"
           ,"Often forget to put things back in their proper place"
           ,"Feel desperate"
           ,"Wait for others to lead the way"
           ,"Seldom get emotional"
           ,"Look down on others"
           ,"Break rules"
           ,"Stumble over my words"
           ,"Like to take it easy"
           ,"Dislike changes"
           ,"Love a good fight"
           ,"Set high standards for myself and others"
           ,"Rarely overindulge"
           ,"Act wild and crazy"
           ,"Am not interested in abstract ideas"
           ,"Think highly of myself"
           ,"Find it difficult to get down to work"
           ,"Remain calm under pressure"
           ,"Look at the bright side of life"
           ,"Believe that too much tax money goes to support artists"
           ,"Tend to dislike soft hearted people"
           ,"Like to act on a whim"
           ,"Am relaxed most of the time"
           ,"Often feel uncomfortable around others"
           ,"Seldom daydream"
           ,"Distrust people"
           ,"Misjudge situations"
           ,"Seldom get mad"
           ,"Want to be left alone"
           ,"Do not like poetry"
           ,"Put people under pressure"
           ,"Leave a mess in my room"
           ,"Feel that my life lacks direction"
           ,"Keep in the background"
           ,"Am not easily affected by my emotions"
           ,"Am indifferent to the feelings of others"
           ,"Break my promises"
           ,"Am not embarassed easily"
           ,"Like to take my time"
           ,"Don't like the idea of change"
           ,"Yell at people"
           ,"Demand quality"
           ,"Easily resist temptations"
           ,"Willing to try anything once"
           ,"Avoid philosophical discussions"
           ,"Have a high opinion of myself"
           ,"Waste my time"
           ,"Can handle complex problems"
           ,"Laugh aloud"
           ,"Believe laws should be strictly enforced"
           ,"Believe in an eye for an eye"
           ,"Rush into things"
           ,"Am not easily disturbed by events"
           ,"Avoid contact with others"
           ,"Do not have a good imagination"
           ,"Suspect hidden motives in others"
           ,"Don't understand things"
           ,"Am not easily annoyed"
           ,"Don't like crowded events"
           ,"Do not enjoy going to art museums"
           ,"Pretend to be concerned for others"
           ,"Leave my belongings around"
           ,"Seldom feel blue"
           ,"Have little to say"
           ,"Rarely notice my emotional reactions"
           ,"Make people feel uncomfortable"
           ,"Get others to do my duties"
           ,"Am comfortable in unfamiliar situations"
           ,"Like a leisurely lifestyle"
           ,"Am a creature of habit"
           ,"Insult people"
           ,"Am not highly motivated to succeed"
           ,"Am able to control my cravings"
           
           
           
           
           ,"Seek danger"
           ,"Have difficulty understanding abstract ideas"
           ,"Know the answers to many questions"
           ,"Need a push to get started"
           ,"Know how to cope"
           ,"Amuse my friends"
           ,"Believe that we coddle criminals too much"
           ,"Try not to think about the needy"
           ,"Do crazy things"
           ,"Don't worry about things that have already happened"
           ,"Am not really interested in others"
           ,"Seldom get lost in thought"
           ,"Am wary of others"
           ,"Have little to contribute"
           ,"Keep my cool"
           ,"Avoid crowds"
           ,"Do not like concerts"
           ,"Take advantage of others"
           ,"Am not bothered by messy people"
           ,"Feel comfortable with myself"
           ,"Don't like to draw attention to myself"
           ,"Experience very few emotional highs and lows"
           ,"Turn my back on others"
           ,"Do the opposite of what is asked"
           ,"Am not bothered by difficult social situations"
           ,"Let things proceed at their own pace"
           ,"Dislike new foods"
           ,"Get back at others"
           ,"Do just enough work to get by"
           ,"Never spend more than I can afford"
           ,"Would never go hand gliding or bungee jumping"
           ,"Am not interested in theoretical discussions"
           ,"Boast about my virtues"
           ,"Have difficulty starting tasks"
           ,"Readily overcome setbacks"
           ,"Am not easily amused"
           ,"Believe that we should be tough on crime"
           ,"Believe people should fend for themselves"
           ,"Act without thinking"
           ,"Adapt easily to new situations"
           ,"Keep others at a distance"
           ,"Have difficulty imagining things"
           ,"Believe that people are essentially evil"
           ,"Don't see the consequences of things"
           ,"Rarely complain"
           ,"Seek quiet"
           ,"Do not enjoy watching dance performances"
           ,"Obstruct others' plans"
           ,"Am not bothered by disorder"
           ,"Am very pleased with myself"
           ,"Hold back my opinions"
           ,"Don't understand people who get emotional"
           ,"Take no time for others"
           ,"Misinterpret the facts"
           ,"Am able t stand up for myself"
           ,"React slowly"
           ,"Am attached to conventional ways"
           ,"Hold a grudge"
           ,"Put little time and effort into my work"
           ,"Never splurge"
           ,"Dislike loud music"
           ,"Avoid difficult reading material"
           ,"Make myself the center of attention"
           ,"Postpone decisions"
           ,"Am calm even in tense situations"
           ,"Seldom joke around"
           ,"Like to stand during the national anthem"
           ,"Can't stand weak people"
           ,"Often make last minute plans")

items2 <- items                 ## inefficient but having difficulty sampling 5 at a time in order
items3 <- items
items4 <- items
items5 <- items
all <- rbind(items,items2,items3,items4,items5)

data$item <- as.factor(data$item)

data$item <- all[c(1:1500)]

## temporary subset:
## plot <- data[1:5,]

## based on Edwards' ratings values:

data$ID <- seq.int(nrow(data))     ## need to multiply by FIVE, then subtract FOUR

## Need to just get in for SIOP 9/11/18; these are old values (sdcorrect_2.csv reordered from other Alicia project)
## sdcorrect.csv had middle items reversed - this should be ok 9/11/18

ed.value <- c(-1.16667,
              3.33333,
              1.66667,
              1.83333,
              3.33333,
              -3.50000,
              1.16667,
              1.00000,
              2.66667,
              1.16667,
              -2.00000,
              1.83333,
              0.00000,
              3.50000,
              2.66667,
              -2.33333,
              0.50000,
              0.16667,
              2.16667,
              2.16667,
              -2.33333,
              0.83333,
              1.50000,
              -0.33333,
              2.33333,
              -2.50000,
              3.16667,
              0.16667,
              2.50000,
              1.50000,
              -1.66667,
              2.33333,
              -0.16667,
              1.83333,
              2.83333,
              -2.83333,
              2.00000,
              0.80000,
              2.33333,
              1.66667,
              -3.00000,
              2.33333,
              2.66667,
              2.33333,
              3.83333,
              -0.66667,
              0.16667,
              1.16667,
              -0.33333,
              3.16667,
              -1.00000,
              0.83333,
              0.66667,
              0.16667,
              2.50000,
              -2.16667,
              2.50000,
              0.50000,
              2.83333,
              2.33333,
              -2.16667,
              2.50000,
              0.00000,
              1.50000,
              2.16667,
              -2.33333,
              1.83333,
              1.66667,
              -1.00000,
              -0.50000,
              -2.00000,
              1.33333,
              2.50000,
              3.83333,
              2.50000,
              -1.66667,
              1.16667,
              1.00000,
              1.33333,
              1.83333,
              -2.33333,
              0.83333,
              1.20000,
              -0.16667,
              2.33333,
              -2.33333,
              1.33333,
              0.16667,
              2.00000,
              -0.16667,
              -2.50000,
              2.33333,
              -0.16667,
              2.16667,
              2.00000,
              -2.83333,
              1.66667,
              0.16667,
              -2.66667,
              0.33333,
              -1.83333,
              0.60000,
              1.33333,
              2.33333,
              3.50000,
              -0.33333,
              1.83333,
              1.50000,
              -2.50000,
              1.83333,
              -2.83333,
              -0.50000,
              2.16667,
              2.00000,
              1.66667,
              -1.33333,
              1.50000,
              -0.50000,
              0.66667,
              -2.00000,
              -2.16667,
              3.16667,
              -0.33333,
              3.00000,
              2.00000,
              -3.00000,
              0.16667,
              1.66667,
              -0.50000,
              1.83333,
              -2.00000,
              1.83333,
              2.00000,
              3.00000,
              3.33333,
              -0.83333,
              0.50000,
              -0.83333,
              -1.33333,
              3.00000,
              -0.66667,
              -1.50000,
              0.33333,
              -2.66667,
              2.00000,
              -1.33333,
              3.00000,
              0.00000,
              -1.83333,
              -0.50000,
              2.16667,
              -1.50000,
              1.50000,
              1.66667,
              2.50000,
              1.83333,
              -1.16667,
              -0.33333,
              -3.50000,
              -1.00000,
              -2.16667,
              -0.50000,
              0.33333,
              -3.00000,
              -1.66667,
              -1.50000,
              0.16667,
              -1.16667,
              -2.00000,
              2.33333,
              1.33333,
              -1.16667,
              -0.50000,
              -0.33333,
              -1.83333,
              2.16667,
              3.00000,
              -0.33333,
              -2.00000,
              -0.33333,
              2.00000,
              -2.33333,
              0.00000,
              -1.33333,
              -1.66667,
              2.50000,
              -1.80000,
              -0.16667,
              -1.40000,
              -0.83333,
              -1.66667,
              -0.83333,
              0.83333,
              -2.83333,
              -3.50000,
              1.00000,
              0.83333,
              -1.16667,
              -2.83333,
              1.83333,
              1.50000,
              1.33333,
              -0.50000,
              -0.66667,
              -2.00000,
              2.33333,
              2.33333,
              0.16667,
              -1.00000,
              -1.33333,
              1.66667,
              -2.33333,
              -0.66667,
              -1.33333,
              -1.66667,
              1.16667,
              -0.66667,
              -0.83333,
              -0.66667,
              -1.40000,
              1.80000,
              -0.80000,
              -0.16667,
              -3.00000,
              -2.33333,
              1.50000,
              0.00000,
              -0.33333,
              -3.66667,
              0.00000,
              2.00000,
              -0.83333,
              -0.83333,
              2.33333,
              -1.33333,
              2.66667,
              1.83333,
              -0.33333,
              -1.50000,
              -1.50000,
              1.16667,
              -2.66667,
              0.33333,
              -1.50000,
              -2.00000,
              3.00000,
              -0.50000,
              -0.50000,
              -3.83333,
              0.66667,
              3.33333,
              0.00000,
              -0.66667,
              -3.66667,
              -3.16667,
              2.33333,
              0.66667,
              -0.83333,
              -2.66667,
              -2.66667,
              2.83333,
              -0.83333,
              0.16667,
              -2.16667,
              -1.50000,
              3.16667,
              -1.50000,
              0.66667,
              -1.33333,
              -2.16667,
              2.83333,
              -2.00000,
              -0.50000,
              -3.33333,
              -2.66667,
              2.66667,
              -0.33333,
              -0.50000,
              -3.00000,
              -0.16667,
              -0.16667,
              -0.16667,
              -1.16667,
              -2.66667,
              -3.16667,
              2.33333,
              -0.83333,
              -1.16667,
              -2.66667,
              -3.16667,
              -0.83333,
              0.00000,
              -1.00000,
              -1.00000,
              -1.33333,
              2.50000,
              -2.16667,
              1.66667,
              -2.00000,
              -0.50000)

use <- rep(ed.value, each=5)

data$Edwards <- NA
data$Edwards <- use

##redo subsetting with new titles

## freq(data$Edwards)
data$Edwards <- round(data$Edwards,2)

sd1.1 <- subset(data, Edwards < -3.4,
                  select=mean:Edwards)
sd2.1 <- subset(data, Edwards == -2,
                select=mean:Edwards)
sd3.1 <- subset(data, Edwards == 0,
                select=mean:Edwards)
sd4.1 <- subset(data, Edwards == 2,
                select=mean:Edwards)
sd5.1 <- subset(data, Edwards > 3.2,
                select=mean:Edwards)

sd1 <- sd1.1[c(1:25),]                    ## highest
sd2 <- sd2.1[c(1:25),]
sd3 <- sd3.1[c(1:25),]                   ## moderate - around zero
sd4 <- sd4.1[c(1:25),]
sd5 <- sd5.1[c(1:25),]          ## lowest

sd1$group <- "highly undesirable"
sd2$group <- "moderately undesirable"
sd3$group <- "neutral"
sd4$group <- "moderately desirable"
sd5$group <- "highly desirable"

one <- rbind(sd1,sd2,sd3,sd4,sd5)

#######################################################################################

##########above looks fine, just need plot - below item texts are incorrect

## write.csv(one,"C:\\Kulas\\kulas.csv")        ## to get item text


one$size_f = factor(one$item, levels=c("Get angry easily"
                                       ,"Cheat to get ahead"
                                       ,"Break my promises"
                                       ,"Insult people"
                                       ,"Take advantage of others"
                                       ,"Often feel blue"
                                       ,"Am often down in the dumps"
                                       ,"Jump into things without thinking"
                                       ,"Have frequent mood swings"
                                       ,"Love a good fight"
                                       ,"Experience my emotions intensely"
                                       ,"Love to daydream"
                                       ,"Tend to vote for conservative political candidates"
                                       ,"Seldom daydream"
                                       ,"Like a leisurely lifestyle"
                                       ,"Talk to a lot of different people at parties"
                                       ,"Value cooperation over competition"
                                       ,"Am sure of my ground"
                                       ,"Seldom toot my own horn"
                                       ,"Come up with good solutions"
                                       ,"Make friends easily"
                                       ,"Complete tasks successfully"
                                       ,"Make people feel welcome"
                                       ,"Keep my promises"
                                       ,"Love to help others"))

one$sd <- factor(one$group, levels = c("highly undesirable","moderately undesirable","neutral",                                                            ## specifies legends ordering
                                       "moderately desirable","highly desirable"))                
one$Edwards <- one$sd   ## 6/3/23 to clarify figure a bit 

d <- ggplot(one, aes(x=level, y=mean, colour=Edwards, group=1)) + geom_line(linewidth=1) +
  scale_fill_discrete(name="Edwards/nCategory") +
scale_x_discrete(limits=c("Extremely High", "Above Average", "Average", "Below Average", "Extremely Low")) +
  facet_wrap(~ size_f, nrow=5) +
  scale_colour_brewer() + theme_dark()
d + theme(axis.text.x = element_text(angle=60, hjust=1)) 

```

### Approach #2: Functional form (Empirical)

Our second approach leveraged hierarchical polynomial regressions, with the index of interest being the change in $R^2$ associated with nonlinear association, as operationalized as a quadratic (second degree) polynomial term. Most item functions ($n_k$ = 203) exhibited a very low change in $R^2$ (see Table \@ref(tab:polytable) for a summary of these results and explicit articulations of our subjective classifications of "low" or "very low").^[$F$-tests associated with the $\Delta R^2$ indicated that 92.31% of item functions were not *significantly* improved when specifying the quadratic term ($\alpha$ = .05). This information is being presented for completeness, although it should be noted that $F$ critical values are very large in this atypical application of regression and effect sizes are more appropriately informative.] These results suggest that, although subjective judgements may be able to visually distinguish functional form[^addthis], empirical estimates are not as sensitive to deviations from linearity.

[^addthis]: Compare with the % of "linear" and maybe "egyptian" that we subjectively categorized - 5/30/24. Also add to discussion -- there is a blurry line between a function being "linear" and "non-linear". Subjectively allowances are given that can be mutually agreed upon. There comes a point, however, where one judge deems a function "linear" and another disagrees. Similarly, there comes a point where both judges agree that the function exhibits non-linearity. Indeed, humans in general may not be very good at dissociating curved and straight lines [@watt1987detection; @gibson1933adaptation; @ogilvie1967perception; @bales1935after].

```{r polytable}

polytable <- read.csv("polytable.csv")[-1,]

polytable$V2 <- formatC( round(as.numeric(polytable$V2), 3 ), format='f', digits=2 )

colnames(polytable)[1] <- "Example Item"
colnames(polytable)[2] <- "$\\Delta R^2$"
colnames(polytable)[3] <- "$\\Delta R^2 Range$"
colnames(polytable)[4] <- "$n_k$"



papaja::apa_table(polytable, # apa contains the data.frame needed for apa_table
                  caption = "Polynomial effects, organized by magnitude",
#                  landscape = TRUE,
                  row.names=FALSE,
                  note = "$\\Delta R^2$ refers to the incremental second-order polynomial effect estimated via hierarchical regression.",
                  escape = FALSE) 

```

### Approach #3: Functional form (Visual)

To supplement the empirical results, Figure \@ref(fig:Figure2) plots for all 300 items were presented to judges who performed an inductive content analysis [@miles1994qualitative] -- grouping item functions by *perceived functional similarity* without further instruction. There were 11 total categories identified, and all but three item functions fit into one of these 11 categories (item functions were assigned exclusively to only one category)[^omitteditems]. Figure \@ref(fig:lastone) presents an exemplar function for each of the 11 categories as well as panel scatterplots and correlations executed within each of the 11 categories. Note that several of the categories represent "mirror" functions (e.g., the "leftmost" category is a symmetrical mirror of the "rightmost" category). The average Edwards rating for all items grouped within each category is presented in Table \@ref(tab:qsorttable) along with the number of items classified as exhibiting the functional form. 

[^omitteditems]: The unclassified item functions were, "Am not interested in other peoples' problems", "Postpone decisions", and "Readily overcome setbacks".

```{r Fig3data}

## Analyses

i1 <- data[c(1:5),]
i2 <- data[c(6:10),]
i3 <- data[c(11:15),]
i4 <- data[c(16:20),]
i5 <- data[c(21:25),]
i6 <- data[c(26:30),]
i7 <- data[c(31:35),]
i8 <- data[c(36:40),]
i9 <- data[c(41:45),]
i10 <- data[c(46:50),]
i11 <- data[c(51:55),]
i12 <- data[c(56:60),]
i13 <- data[c(61:65),]
i14 <- data[c(66:70),]
i15 <- data[c(71:75),]
i16 <- data[c(76:80),]
i17 <- data[c(81:85),]
i18 <- data[c(86:90),]
i19 <- data[c(91:95),]
i20 <- data[c(96:100),]

i21 <- data[c(101:105),]
i22 <- data[c(106:110),]
i23 <- data[c(111:115),]
i24 <- data[c(116:120),]
i25 <- data[c(121:125),]
i26 <- data[c(126:130),]
i27 <- data[c(131:135),]
i28 <- data[c(136:140),]
i29 <- data[c(141:145),]
i30 <- data[c(146:150),]
i31 <- data[c(151:155),]
i32 <- data[c(156:160),]
i33 <- data[c(161:165),]
i34 <- data[c(166:170),]
i35 <- data[c(171:175),]
i36 <- data[c(176:180),]
i37 <- data[c(181:185),]
i38 <- data[c(186:190),]
i39 <- data[c(191:195),]
i40 <- data[c(196:200),]

i41 <- data[c(201:205),]
i42 <- data[c(206:210),]
i43 <- data[c(211:215),]
i44 <- data[c(216:220),]
i45 <- data[c(221:225),]
i46 <- data[c(226:230),]
i47 <- data[c(231:235),]
i48 <- data[c(236:240),]
i49 <- data[c(241:245),]
i50 <- data[c(246:250),]
i51 <- data[c(251:255),]
i52 <- data[c(256:260),]
i53 <- data[c(261:265),]
i54 <- data[c(266:270),]
i55 <- data[c(271:275),]
i56 <- data[c(276:280),]
i57 <- data[c(281:285),]
i58 <- data[c(286:290),]
i59 <- data[c(291:295),]
i60 <- data[c(296:300),]

i61 <- data[c(301:305),]
i62 <- data[c(306:310),]
i63 <- data[c(311:315),]
i64 <- data[c(316:320),]
i65 <- data[c(321:325),]
i66 <- data[c(326:330),]
i67 <- data[c(331:335),]
i68 <- data[c(336:340),]
i69 <- data[c(341:345),]
i70 <- data[c(346:350),]
i71 <- data[c(351:355),]
i72 <- data[c(356:360),]
i73 <- data[c(361:365),]
i74 <- data[c(366:370),]
i75 <- data[c(371:375),]
i76 <- data[c(376:380),]
i77 <- data[c(381:385),]
i78 <- data[c(386:390),]
i79 <- data[c(391:395),]
i80 <- data[c(396:400),]

i81 <- data[c(401:405),]
i82 <- data[c(406:410),]
i83 <- data[c(411:415),]
i84 <- data[c(416:420),]
i85 <- data[c(421:425),]
i86 <- data[c(426:430),]
i87 <- data[c(431:435),]
i88 <- data[c(436:440),]
i89 <- data[c(441:445),]
i90 <- data[c(446:450),]
i91 <- data[c(451:455),]
i92 <- data[c(456:460),]
i93 <- data[c(461:465),]
i94 <- data[c(466:470),]
i95 <- data[c(471:475),]
i96 <- data[c(476:480),]
i97 <- data[c(481:485),]
i98 <- data[c(486:490),]
i99 <- data[c(491:495),]
i100 <- data[c(496:500),]



i101 <- data[c(501:505),]
i102 <- data[c(506:510),]
i103 <- data[c(511:515),]
i104 <- data[c(516:520),]
i105 <- data[c(521:525),]
i106 <- data[c(526:530),]
i107 <- data[c(531:535),]
i108 <- data[c(536:540),]
i109 <- data[c(541:545),]
i110 <- data[c(546:550),]
i111 <- data[c(551:555),]
i112 <- data[c(556:560),]
i113 <- data[c(561:565),]
i114 <- data[c(566:570),]
i115 <- data[c(571:575),]
i116 <- data[c(576:580),]
i117 <- data[c(581:585),]
i118 <- data[c(586:590),]
i119 <- data[c(591:595),]
i120 <- data[c(596:600),]

i121 <- data[c(601:605),]
i122 <- data[c(606:610),]
i123 <- data[c(611:615),]
i124 <- data[c(616:620),]
i125 <- data[c(621:625),]
i126 <- data[c(626:630),]
i127 <- data[c(631:635),]
i128 <- data[c(636:640),]
i129 <- data[c(641:645),]
i130 <- data[c(646:650),]
i131 <- data[c(651:655),]
i132 <- data[c(656:660),]
i133 <- data[c(661:665),]
i134 <- data[c(666:670),]
i135 <- data[c(671:675),]
i136 <- data[c(676:680),]
i137 <- data[c(681:685),]
i138 <- data[c(686:690),]
i139 <- data[c(691:695),]
i140 <- data[c(696:700),]

i141 <- data[c(701:705),]
i142 <- data[c(706:710),]
i143 <- data[c(711:715),]
i144 <- data[c(716:720),]
i145 <- data[c(721:725),]
i146 <- data[c(726:730),]
i147 <- data[c(731:735),]
i148 <- data[c(736:740),]
i149 <- data[c(741:745),]
i150 <- data[c(746:750),]
i151 <- data[c(751:755),]
i152 <- data[c(756:760),]
i153 <- data[c(761:765),]
i154 <- data[c(766:770),]
i155 <- data[c(771:775),]
i156 <- data[c(776:780),]
i157 <- data[c(781:785),]
i158 <- data[c(786:790),]
i159 <- data[c(791:795),]
i160 <- data[c(796:800),]

i161 <- data[c(801:805),]
i162 <- data[c(806:810),]
i163 <- data[c(811:815),]
i164 <- data[c(816:820),]
i165 <- data[c(821:825),]
i166 <- data[c(826:830),]
i167 <- data[c(831:835),]
i168 <- data[c(836:840),]
i169 <- data[c(841:845),]
i170 <- data[c(846:850),]
i171 <- data[c(851:855),]
i172 <- data[c(856:860),]
i173 <- data[c(861:865),]
i174 <- data[c(866:870),]
i175 <- data[c(871:875),]
i176 <- data[c(876:880),]
i177 <- data[c(881:885),]
i178 <- data[c(886:890),]
i179 <- data[c(891:895),]
i180 <- data[c(896:900),]

i181 <- data[c(901:905),]
i182 <- data[c(906:910),]
i183 <- data[c(911:915),]
i184 <- data[c(916:920),]
i185 <- data[c(921:925),]
i186 <- data[c(926:930),]
i187 <- data[c(931:935),]
i188 <- data[c(936:940),]
i189 <- data[c(941:945),]
i190 <- data[c(946:950),]
i191 <- data[c(951:955),]
i192 <- data[c(956:960),]
i193 <- data[c(961:965),]
i194 <- data[c(966:970),]
i195 <- data[c(971:975),]
i196 <- data[c(976:980),]
i197 <- data[c(981:985),]
i198 <- data[c(986:990),]
i199 <- data[c(991:995),]
i200 <- data[c(996:1000),]


i201 <- data[c(1001:1005),]
i202 <- data[c(1006:1010),]
i203 <- data[c(1011:1015),]
i204 <- data[c(1016:1020),]
i205 <- data[c(1021:1025),]
i206 <- data[c(1026:1030),]
i207 <- data[c(1031:1035),]
i208 <- data[c(1036:1040),]
i209 <- data[c(1041:1045),]
i210 <- data[c(1046:1050),]
i211 <- data[c(1051:1055),]
i212 <- data[c(1056:1060),]
i213 <- data[c(1061:1065),]
i214 <- data[c(1066:1070),]
i215 <- data[c(1071:1075),]
i216 <- data[c(1076:1080),]
i217 <- data[c(1081:1085),]
i218 <- data[c(1086:1090),]
i219 <- data[c(1091:1095),]
i220 <- data[c(1096:1100),]
i221 <- data[c(1101:1105),]
i222 <- data[c(1106:1110),]
i223 <- data[c(1111:1115),]
i224 <- data[c(1116:1120),]
i225 <- data[c(1121:1125),]
i226 <- data[c(1126:1130),]
i227 <- data[c(1131:1135),]
i228 <- data[c(1136:1140),]
i229 <- data[c(1141:1145),]
i230 <- data[c(1146:1150),]
i231 <- data[c(1151:1155),]
i232 <- data[c(1156:1160),]
i233 <- data[c(1161:1165),]
i234 <- data[c(1166:1170),]
i235 <- data[c(1171:1175),]
i236 <- data[c(1176:1180),]
i237 <- data[c(1181:1185),]
i238 <- data[c(1186:1190),]
i239 <- data[c(1191:1195),]
i240 <- data[c(1196:1200),]
i241 <- data[c(1201:1205),]
i242 <- data[c(1206:1210),]
i243 <- data[c(1211:1215),]
i244 <- data[c(1216:1220),]
i245 <- data[c(1221:1225),]
i246 <- data[c(1226:1230),]
i247 <- data[c(1231:1235),]
i248 <- data[c(1236:1240),]
i249 <- data[c(1241:1245),]
i250 <- data[c(1246:1250),]
i251 <- data[c(1251:1255),]
i252 <- data[c(1256:1260),]
i253 <- data[c(1261:1265),]
i254 <- data[c(1266:1270),]
i255 <- data[c(1271:1275),]
i256 <- data[c(1276:1280),]
i257 <- data[c(1281:1285),]
i258 <- data[c(1286:1290),]
i259 <- data[c(1291:1295),]
i260 <- data[c(1296:1300),]
i261 <- data[c(1301:1305),]
i262 <- data[c(1306:1310),]
i263 <- data[c(1311:1315),]
i264 <- data[c(1316:1320),]
i265 <- data[c(1321:1325),]
i266 <- data[c(1326:1330),]
i267 <- data[c(1331:1335),]
i268 <- data[c(1336:1340),]
i269 <- data[c(1341:1345),]
i270 <- data[c(1346:1350),]
i271 <- data[c(1351:1355),]
i272 <- data[c(1356:1360),]
i273 <- data[c(1361:1365),]
i274 <- data[c(1366:1370),]
i275 <- data[c(1371:1375),]
i276 <- data[c(1376:1380),]
i277 <- data[c(1381:1385),]
i278 <- data[c(1386:1390),]
i279 <- data[c(1391:1395),]
i280 <- data[c(1396:1400),]
i281 <- data[c(1401:1405),]
i282 <- data[c(1406:1410),]
i283 <- data[c(1411:1415),]
i284 <- data[c(1416:1420),]
i285 <- data[c(1421:1425),]
i286 <- data[c(1426:1430),]
i287 <- data[c(1431:1435),]
i288 <- data[c(1436:1440),]
i289 <- data[c(1441:1445),]
i290 <- data[c(1446:1450),]
i291 <- data[c(1451:1455),]
i292 <- data[c(1456:1460),]
i293 <- data[c(1461:1465),]
i294 <- data[c(1466:1470),]
i295 <- data[c(1471:1475),]
i296 <- data[c(1476:1480),]
i297 <- data[c(1481:1485),]
i298 <- data[c(1486:1490),]
i299 <- data[c(1491:1495),]
i300 <- data[c(1496:1500),]


r1 <- lm(mean ~ ID, i1)
r2 <- lm(mean ~ ID, i2)
r3 <- lm(mean ~ ID, i3)
r4 <- lm(mean ~ ID, i4)
r5 <- lm(mean ~ ID, i5)
r6 <- lm(mean ~ ID, i6)
r7 <- lm(mean ~ ID, i7)
r8 <- lm(mean ~ ID, i8)
r9 <- lm(mean ~ ID, i9)
r10 <- lm(mean ~ ID, i10)
r11 <- lm(mean ~ ID, i11)
r12 <- lm(mean ~ ID, i12)
r13 <- lm(mean ~ ID, i13)
r14 <- lm(mean ~ ID, i14)
r15 <- lm(mean ~ ID, i15)
r16 <- lm(mean ~ ID, i16)
r17 <- lm(mean ~ ID, i17)
r18 <- lm(mean ~ ID, i18)
r19 <- lm(mean ~ ID, i19)
r20 <- lm(mean ~ ID, i20)
r21 <- lm(mean ~ ID, i21)
r22 <- lm(mean ~ ID, i22)
r23 <- lm(mean ~ ID, i23)
r24 <- lm(mean ~ ID, i24)
r25 <- lm(mean ~ ID, i25)
r26 <- lm(mean ~ ID, i26)
r27 <- lm(mean ~ ID, i27)
r28 <- lm(mean ~ ID, i28)
r29 <- lm(mean ~ ID, i29)
r30 <- lm(mean ~ ID, i30)
r31 <- lm(mean ~ ID, i31)
r32 <- lm(mean ~ ID, i32)
r33 <- lm(mean ~ ID, i33)
r34 <- lm(mean ~ ID, i34)
r35 <- lm(mean ~ ID, i35)
r36 <- lm(mean ~ ID, i36)
r37 <- lm(mean ~ ID, i37)
r38 <- lm(mean ~ ID, i38)
r39 <- lm(mean ~ ID, i39)
r40 <- lm(mean ~ ID, i40)
r41 <- lm(mean ~ ID, i41)
r42 <- lm(mean ~ ID, i42)
r43 <- lm(mean ~ ID, i43)
r44 <- lm(mean ~ ID, i44)
r45 <- lm(mean ~ ID, i45)
r46 <- lm(mean ~ ID, i46)
r47 <- lm(mean ~ ID, i47)
r48 <- lm(mean ~ ID, i48)
r49 <- lm(mean ~ ID, i49)
r50 <- lm(mean ~ ID, i50)
r51 <- lm(mean ~ ID, i51)
r52 <- lm(mean ~ ID, i52)
r53 <- lm(mean ~ ID, i53)
r54 <- lm(mean ~ ID, i54)
r55 <- lm(mean ~ ID, i55)
r56 <- lm(mean ~ ID, i56)
r57 <- lm(mean ~ ID, i57)
r58 <- lm(mean ~ ID, i58)
r59 <- lm(mean ~ ID, i59)
r60 <- lm(mean ~ ID, i60)
r61 <- lm(mean ~ ID, i61)
r62 <- lm(mean ~ ID, i62)
r63 <- lm(mean ~ ID, i63)
r64 <- lm(mean ~ ID, i64)
r65 <- lm(mean ~ ID, i65)
r66 <- lm(mean ~ ID, i66)
r67 <- lm(mean ~ ID, i67)
r68 <- lm(mean ~ ID, i68)
r69 <- lm(mean ~ ID, i69)
r70 <- lm(mean ~ ID, i70)
r71 <- lm(mean ~ ID, i71)
r72 <- lm(mean ~ ID, i72)
r73 <- lm(mean ~ ID, i73)
r74 <- lm(mean ~ ID, i74)
r75 <- lm(mean ~ ID, i75)
r76 <- lm(mean ~ ID, i76)
r77 <- lm(mean ~ ID, i77)
r78 <- lm(mean ~ ID, i78)
r79 <- lm(mean ~ ID, i79)
r80 <- lm(mean ~ ID, i80)
r81 <- lm(mean ~ ID, i81)
r82 <- lm(mean ~ ID, i82)
r83 <- lm(mean ~ ID, i83)
r84 <- lm(mean ~ ID, i84)
r85 <- lm(mean ~ ID, i85)
r86 <- lm(mean ~ ID, i86)
r87 <- lm(mean ~ ID, i87)
r88 <- lm(mean ~ ID, i88)
r89 <- lm(mean ~ ID, i89)
r90 <- lm(mean ~ ID, i90)
r91 <- lm(mean ~ ID, i91)
r92 <- lm(mean ~ ID, i92)
r93 <- lm(mean ~ ID, i93)
r94 <- lm(mean ~ ID, i94)
r95 <- lm(mean ~ ID, i95)
r96 <- lm(mean ~ ID, i96)
r97 <- lm(mean ~ ID, i97)
r98 <- lm(mean ~ ID, i98)
r99 <- lm(mean ~ ID, i99)
r100 <- lm(mean ~ ID, i100)
r101 <- lm(mean ~ ID, i101)
r102 <- lm(mean ~ ID, i102)
r103 <- lm(mean ~ ID, i103)
r104 <- lm(mean ~ ID, i104)
r105 <- lm(mean ~ ID, i105)
r106 <- lm(mean ~ ID, i106)
r107 <- lm(mean ~ ID, i107)
r108 <- lm(mean ~ ID, i108)
r109 <- lm(mean ~ ID, i109)
r110 <- lm(mean ~ ID, i110)
r111 <- lm(mean ~ ID, i111)
r112 <- lm(mean ~ ID, i112)
r113 <- lm(mean ~ ID, i113)
r114 <- lm(mean ~ ID, i114)
r115 <- lm(mean ~ ID, i115)
r116 <- lm(mean ~ ID, i116)
r117 <- lm(mean ~ ID, i117)
r118 <- lm(mean ~ ID, i118)
r119 <- lm(mean ~ ID, i119)
r120 <- lm(mean ~ ID, i120)
r121 <- lm(mean ~ ID, i121)
r122 <- lm(mean ~ ID, i122)
r123 <- lm(mean ~ ID, i123)
r124 <- lm(mean ~ ID, i124)
r125 <- lm(mean ~ ID, i125)
r126 <- lm(mean ~ ID, i126)
r127 <- lm(mean ~ ID, i127)
r128 <- lm(mean ~ ID, i128)
r129 <- lm(mean ~ ID, i129)
r130 <- lm(mean ~ ID, i130)
r131 <- lm(mean ~ ID, i131)
r132 <- lm(mean ~ ID, i132)
r133 <- lm(mean ~ ID, i133)
r134 <- lm(mean ~ ID, i134)
r135 <- lm(mean ~ ID, i135)
r136 <- lm(mean ~ ID, i136)
r137 <- lm(mean ~ ID, i137)
r138 <- lm(mean ~ ID, i138)
r139 <- lm(mean ~ ID, i139)
r140 <- lm(mean ~ ID, i140)
r141 <- lm(mean ~ ID, i141)
r142 <- lm(mean ~ ID, i142)
r143 <- lm(mean ~ ID, i143)
r144 <- lm(mean ~ ID, i144)
r145 <- lm(mean ~ ID, i145)
r146 <- lm(mean ~ ID, i146)
r147 <- lm(mean ~ ID, i147)
r148 <- lm(mean ~ ID, i148)
r149 <- lm(mean ~ ID, i149)
r150 <- lm(mean ~ ID, i150)
r151 <- lm(mean ~ ID, i151)
r152 <- lm(mean ~ ID, i152)
r153 <- lm(mean ~ ID, i153)
r154 <- lm(mean ~ ID, i154)
r155 <- lm(mean ~ ID, i155)
r156 <- lm(mean ~ ID, i156)
r157 <- lm(mean ~ ID, i157)
r158 <- lm(mean ~ ID, i158)
r159 <- lm(mean ~ ID, i159)
r160 <- lm(mean ~ ID, i160)
r161 <- lm(mean ~ ID, i161)
r162 <- lm(mean ~ ID, i162)
r163 <- lm(mean ~ ID, i163)
r164 <- lm(mean ~ ID, i164)
r165 <- lm(mean ~ ID, i165)
r166 <- lm(mean ~ ID, i166)
r167 <- lm(mean ~ ID, i167)
r168 <- lm(mean ~ ID, i168)
r169 <- lm(mean ~ ID, i169)
r170 <- lm(mean ~ ID, i170)
r171 <- lm(mean ~ ID, i171)
r172 <- lm(mean ~ ID, i172)
r173 <- lm(mean ~ ID, i173)
r174 <- lm(mean ~ ID, i174)
r175 <- lm(mean ~ ID, i175)
r176 <- lm(mean ~ ID, i176)
r177 <- lm(mean ~ ID, i177)
r178 <- lm(mean ~ ID, i178)
r179 <- lm(mean ~ ID, i179)
r180 <- lm(mean ~ ID, i180)
r181 <- lm(mean ~ ID, i181)
r182 <- lm(mean ~ ID, i182)
r183 <- lm(mean ~ ID, i183)
r184 <- lm(mean ~ ID, i184)
r185 <- lm(mean ~ ID, i185)
r186 <- lm(mean ~ ID, i186)
r187 <- lm(mean ~ ID, i187)
r188 <- lm(mean ~ ID, i188)
r189 <- lm(mean ~ ID, i189)
r190 <- lm(mean ~ ID, i190)
r191 <- lm(mean ~ ID, i191)
r192 <- lm(mean ~ ID, i192)
r193 <- lm(mean ~ ID, i193)
r194 <- lm(mean ~ ID, i194)
r195 <- lm(mean ~ ID, i195)
r196 <- lm(mean ~ ID, i196)
r197 <- lm(mean ~ ID, i197)
r198 <- lm(mean ~ ID, i198)
r199 <- lm(mean ~ ID, i199)
r200 <- lm(mean ~ ID, i200)
r201 <- lm(mean ~ ID, i201)
r202 <- lm(mean ~ ID, i202)
r203 <- lm(mean ~ ID, i203)
r204 <- lm(mean ~ ID, i204)
r205 <- lm(mean ~ ID, i205)
r206 <- lm(mean ~ ID, i206)
r207 <- lm(mean ~ ID, i207)
r208 <- lm(mean ~ ID, i208)
r209 <- lm(mean ~ ID, i209)
r210 <- lm(mean ~ ID, i210)
r211 <- lm(mean ~ ID, i211)
r212 <- lm(mean ~ ID, i212)
r213 <- lm(mean ~ ID, i213)
r214 <- lm(mean ~ ID, i214)
r215 <- lm(mean ~ ID, i215)
r216 <- lm(mean ~ ID, i216)
r217 <- lm(mean ~ ID, i217)
r218 <- lm(mean ~ ID, i218)
r219 <- lm(mean ~ ID, i219)
r220 <- lm(mean ~ ID, i220)
r221 <- lm(mean ~ ID, i221)
r222 <- lm(mean ~ ID, i222)
r223 <- lm(mean ~ ID, i223)
r224 <- lm(mean ~ ID, i224)
r225 <- lm(mean ~ ID, i225)
r226 <- lm(mean ~ ID, i226)
r227 <- lm(mean ~ ID, i227)
r228 <- lm(mean ~ ID, i228)
r229 <- lm(mean ~ ID, i229)
r230 <- lm(mean ~ ID, i230)
r231 <- lm(mean ~ ID, i231)
r232 <- lm(mean ~ ID, i232)
r233 <- lm(mean ~ ID, i233)
r234 <- lm(mean ~ ID, i234)
r235 <- lm(mean ~ ID, i235)
r236 <- lm(mean ~ ID, i236)
r237 <- lm(mean ~ ID, i237)
r238 <- lm(mean ~ ID, i238)
r239 <- lm(mean ~ ID, i239)
r240 <- lm(mean ~ ID, i240)
r241 <- lm(mean ~ ID, i241)
r242 <- lm(mean ~ ID, i242)
r243 <- lm(mean ~ ID, i243)
r244 <- lm(mean ~ ID, i244)
r245 <- lm(mean ~ ID, i245)
r246 <- lm(mean ~ ID, i246)
r247 <- lm(mean ~ ID, i247)
r248 <- lm(mean ~ ID, i248)
r249 <- lm(mean ~ ID, i249)
r250 <- lm(mean ~ ID, i250)
r251 <- lm(mean ~ ID, i251)
r252 <- lm(mean ~ ID, i252)
r253 <- lm(mean ~ ID, i253)
r254 <- lm(mean ~ ID, i254)
r255 <- lm(mean ~ ID, i255)
r256 <- lm(mean ~ ID, i256)
r257 <- lm(mean ~ ID, i257)
r258 <- lm(mean ~ ID, i258)
r259 <- lm(mean ~ ID, i259)
r260 <- lm(mean ~ ID, i260)
r261 <- lm(mean ~ ID, i261)
r262 <- lm(mean ~ ID, i262)
r263 <- lm(mean ~ ID, i263)
r264 <- lm(mean ~ ID, i264)
r265 <- lm(mean ~ ID, i265)
r266 <- lm(mean ~ ID, i266)
r267 <- lm(mean ~ ID, i267)
r268 <- lm(mean ~ ID, i268)
r269 <- lm(mean ~ ID, i269)
r270 <- lm(mean ~ ID, i270)
r271 <- lm(mean ~ ID, i271)
r272 <- lm(mean ~ ID, i272)
r273 <- lm(mean ~ ID, i273)
r274 <- lm(mean ~ ID, i274)
r275 <- lm(mean ~ ID, i275)
r276 <- lm(mean ~ ID, i276)
r277 <- lm(mean ~ ID, i277)
r278 <- lm(mean ~ ID, i278)
r279 <- lm(mean ~ ID, i279)
r280 <- lm(mean ~ ID, i280)
r281 <- lm(mean ~ ID, i281)
r282 <- lm(mean ~ ID, i282)
r283 <- lm(mean ~ ID, i283)
r284 <- lm(mean ~ ID, i284)
r285 <- lm(mean ~ ID, i285)
r286 <- lm(mean ~ ID, i286)
r287 <- lm(mean ~ ID, i287)
r288 <- lm(mean ~ ID, i288)
r289 <- lm(mean ~ ID, i289)
r290 <- lm(mean ~ ID, i290)
r291 <- lm(mean ~ ID, i291)
r292 <- lm(mean ~ ID, i292)
r293 <- lm(mean ~ ID, i293)
r294 <- lm(mean ~ ID, i294)
r295 <- lm(mean ~ ID, i295)
r296 <- lm(mean ~ ID, i296)
r297 <- lm(mean ~ ID, i297)
r298 <- lm(mean ~ ID, i298)
r299 <- lm(mean ~ ID, i299)
r300 <- lm(mean ~ ID, i300)

c1 <- r1$coefficients
c2 <- r2$coefficients
c3 <- r3$coefficients
c4 <- r4$coefficients
c5 <- r5$coefficients
c6 <- r6$coefficients
c7 <- r7$coefficients
c8 <- r8$coefficients
c9 <- r9$coefficients
c10 <- r10$coefficients
c11 <- r11$coefficients
c12 <- r12$coefficients
c13 <- r13$coefficients
c14 <- r14$coefficients
c15 <- r15$coefficients
c16 <- r16$coefficients
c17 <- r17$coefficients
c18 <- r18$coefficients
c19 <- r19$coefficients
c20 <- r20$coefficients
c21 <- r21$coefficients
c22 <- r22$coefficients
c23 <- r23$coefficients
c24 <- r24$coefficients
c25 <- r25$coefficients
c26 <- r26$coefficients
c27 <- r27$coefficients
c28 <- r28$coefficients
c29 <- r29$coefficients
c30 <- r30$coefficients
c31 <- r31$coefficients
c32 <- r32$coefficients
c33 <- r33$coefficients
c34 <- r34$coefficients
c35 <- r35$coefficients
c36 <- r36$coefficients
c37 <- r37$coefficients
c38 <- r38$coefficients
c39 <- r39$coefficients
c40 <- r40$coefficients
c41 <- r41$coefficients
c42 <- r42$coefficients
c43 <- r43$coefficients
c44 <- r44$coefficients
c45 <- r45$coefficients
c46 <- r46$coefficients
c47 <- r47$coefficients
c48 <- r48$coefficients
c49 <- r49$coefficients
c50 <- r50$coefficients
c51 <- r51$coefficients
c52 <- r52$coefficients
c53 <- r53$coefficients
c54 <- r54$coefficients
c55 <- r55$coefficients
c56 <- r56$coefficients
c57 <- r57$coefficients
c58 <- r58$coefficients
c59 <- r59$coefficients
c60 <- r60$coefficients
c61 <- r61$coefficients
c62 <- r62$coefficients
c63 <- r63$coefficients
c64 <- r64$coefficients
c65 <- r65$coefficients
c66 <- r66$coefficients
c67 <- r67$coefficients
c68 <- r68$coefficients
c69 <- r69$coefficients
c70 <- r70$coefficients
c71 <- r71$coefficients
c72 <- r72$coefficients
c73 <- r73$coefficients
c74 <- r74$coefficients
c75 <- r75$coefficients
c76 <- r76$coefficients
c77 <- r77$coefficients
c78 <- r78$coefficients
c79 <- r79$coefficients
c80 <- r80$coefficients
c81 <- r81$coefficients
c82 <- r82$coefficients
c83 <- r83$coefficients
c84 <- r84$coefficients
c85 <- r85$coefficients
c86 <- r86$coefficients
c87 <- r87$coefficients
c88 <- r88$coefficients
c89 <- r89$coefficients
c90 <- r90$coefficients
c91 <- r91$coefficients
c92 <- r92$coefficients
c93 <- r93$coefficients
c94 <- r94$coefficients
c95 <- r95$coefficients
c96 <- r96$coefficients
c97 <- r97$coefficients
c98 <- r98$coefficients
c99 <- r99$coefficients
c100 <- r100$coefficients

c101 <- r101$coefficients
c102 <- r102$coefficients
c103 <- r103$coefficients
c104 <- r104$coefficients
c105 <- r105$coefficients
c106 <- r106$coefficients
c107 <- r107$coefficients
c108 <- r108$coefficients
c109 <- r109$coefficients
c110 <- r110$coefficients
c111 <- r111$coefficients
c112 <- r112$coefficients
c113 <- r113$coefficients
c114 <- r114$coefficients
c115 <- r115$coefficients
c116 <- r116$coefficients
c117 <- r117$coefficients
c118 <- r118$coefficients
c119 <- r119$coefficients
c120 <- r120$coefficients
c121 <- r121$coefficients
c122 <- r122$coefficients
c123 <- r123$coefficients
c124 <- r124$coefficients
c125 <- r125$coefficients
c126 <- r126$coefficients
c127 <- r127$coefficients
c128 <- r128$coefficients
c129 <- r129$coefficients
c130 <- r130$coefficients
c131 <- r131$coefficients
c132 <- r132$coefficients
c133 <- r133$coefficients
c134 <- r134$coefficients
c135 <- r135$coefficients
c136 <- r136$coefficients
c137 <- r137$coefficients
c138 <- r138$coefficients
c139 <- r139$coefficients
c140 <- r140$coefficients
c141 <- r141$coefficients
c142 <- r142$coefficients
c143 <- r143$coefficients
c144 <- r144$coefficients
c145 <- r145$coefficients
c146 <- r146$coefficients
c147 <- r147$coefficients
c148 <- r148$coefficients
c149 <- r149$coefficients
c150 <- r150$coefficients
c151 <- r151$coefficients
c152 <- r152$coefficients
c153 <- r153$coefficients
c154 <- r154$coefficients
c155 <- r155$coefficients
c156 <- r156$coefficients
c157 <- r157$coefficients
c158 <- r158$coefficients
c159 <- r159$coefficients
c160 <- r160$coefficients
c161 <- r161$coefficients
c162 <- r162$coefficients
c163 <- r163$coefficients
c164 <- r164$coefficients
c165 <- r165$coefficients
c166 <- r166$coefficients
c167 <- r167$coefficients
c168 <- r168$coefficients
c169 <- r169$coefficients
c170 <- r170$coefficients
c171 <- r171$coefficients
c172 <- r172$coefficients
c173 <- r173$coefficients
c174 <- r174$coefficients
c175 <- r175$coefficients
c176 <- r176$coefficients
c177 <- r177$coefficients
c178 <- r178$coefficients
c179 <- r179$coefficients
c180 <- r180$coefficients
c181 <- r181$coefficients
c182 <- r182$coefficients
c183 <- r183$coefficients
c184 <- r184$coefficients
c185 <- r185$coefficients
c186 <- r186$coefficients
c187 <- r187$coefficients
c188 <- r188$coefficients
c189 <- r189$coefficients
c190 <- r190$coefficients
c191 <- r191$coefficients
c192 <- r192$coefficients
c193 <- r193$coefficients
c194 <- r194$coefficients
c195 <- r195$coefficients
c196 <- r196$coefficients
c197 <- r197$coefficients
c198 <- r198$coefficients
c199 <- r199$coefficients
c200 <- r200$coefficients

c201 <- r201$coefficients
c202 <- r202$coefficients
c203 <- r203$coefficients
c204 <- r204$coefficients
c205 <- r205$coefficients
c206 <- r206$coefficients
c207 <- r207$coefficients
c208 <- r208$coefficients
c209 <- r209$coefficients
c210 <- r210$coefficients
c211 <- r211$coefficients
c212 <- r212$coefficients
c213 <- r213$coefficients
c214 <- r214$coefficients
c215 <- r215$coefficients
c216 <- r216$coefficients
c217 <- r217$coefficients
c218 <- r218$coefficients
c219 <- r219$coefficients
c220 <- r220$coefficients
c221 <- r221$coefficients
c222 <- r222$coefficients
c223 <- r223$coefficients
c224 <- r224$coefficients
c225 <- r225$coefficients
c226 <- r226$coefficients
c227 <- r227$coefficients
c228 <- r228$coefficients
c229 <- r229$coefficients
c230 <- r230$coefficients
c231 <- r231$coefficients
c232 <- r232$coefficients
c233 <- r233$coefficients
c234 <- r234$coefficients
c235 <- r235$coefficients
c236 <- r236$coefficients
c237 <- r237$coefficients
c238 <- r238$coefficients
c239 <- r239$coefficients
c240 <- r240$coefficients
c241 <- r241$coefficients
c242 <- r242$coefficients
c243 <- r243$coefficients
c244 <- r244$coefficients
c245 <- r245$coefficients
c246 <- r246$coefficients
c247 <- r247$coefficients
c248 <- r248$coefficients
c249 <- r249$coefficients
c250 <- r250$coefficients
c251 <- r251$coefficients
c252 <- r252$coefficients
c253 <- r253$coefficients
c254 <- r254$coefficients
c255 <- r255$coefficients
c256 <- r256$coefficients
c257 <- r257$coefficients
c258 <- r258$coefficients
c259 <- r259$coefficients
c260 <- r260$coefficients
c261 <- r261$coefficients
c262 <- r262$coefficients
c263 <- r263$coefficients
c264 <- r264$coefficients
c265 <- r265$coefficients
c266 <- r266$coefficients
c267 <- r267$coefficients
c268 <- r268$coefficients
c269 <- r269$coefficients
c270 <- r270$coefficients
c271 <- r271$coefficients
c272 <- r272$coefficients
c273 <- r273$coefficients
c274 <- r274$coefficients
c275 <- r275$coefficients
c276 <- r276$coefficients
c277 <- r277$coefficients
c278 <- r278$coefficients
c279 <- r279$coefficients
c280 <- r280$coefficients
c281 <- r281$coefficients
c282 <- r282$coefficients
c283 <- r283$coefficients
c284 <- r284$coefficients
c285 <- r285$coefficients
c286 <- r286$coefficients
c287 <- r287$coefficients
c288 <- r288$coefficients
c289 <- r289$coefficients
c290 <- r290$coefficients
c291 <- r291$coefficients
c292 <- r292$coefficients
c293 <- r293$coefficients
c294 <- r294$coefficients
c295 <- r295$coefficients
c296 <- r296$coefficients
c297 <- r297$coefficients
c298 <- r298$coefficients
c299 <- r299$coefficients
c300 <- r300$coefficients

big.reg <- as.data.frame(rbind(c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,
		c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,
		c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,
		c31,c32,c33,c34,c35,c36,c37,c38,c39,c40,
		c41,c42,c43,c44,c45,c46,c47,c48,c49,c50,
		c51,c52,c53,c54,c55,c56,c57,c58,c59,c60,
		c61,c62,c63,c64,c65,c66,c67,c68,c69,c70,
		c71,c72,c73,c74,c75,c76,c77,c78,c79,c80,
		c81,c82,c83,c84,c85,c86,c87,c88,c89,c90,
		c91,c92,c93,c94,c95,c96,c97,c98,c99,c100,
		c101,c102,c103,c104,c105,c106,c107,c108,c109,c110,
		c111,c112,c113,c114,c115,c116,c117,c118,c119,c120,
		c121,c122,c123,c124,c125,c126,c127,c128,c129,c130,
		c131,c132,c133,c134,c135,c136,c137,c138,c139,c140,
		c141,c142,c143,c144,c145,c146,c147,c148,c149,c150,
		c151,c152,c153,c154,c155,c156,c157,c158,c159,c160,
		c161,c162,c163,c164,c165,c166,c167,c168,c169,c170,
		c171,c172,c173,c174,c175,c176,c177,c178,c179,c180,
		c181,c182,c183,c184,c185,c186,c187,c188,c189,c190,
		c191,c192,c193,c194,c195,c196,c197,c198,c199,c200,
		c201,c202,c203,c204,c205,c206,c207,c208,c209,c210,
		c211,c212,c213,c214,c215,c216,c217,c218,c219,c220,
		c221,c222,c223,c224,c225,c226,c227,c228,c229,c230,
		c231,c232,c233,c234,c235,c236,c237,c238,c239,c240,
		c241,c242,c243,c244,c245,c246,c247,c248,c249,c250,
		c251,c252,c253,c254,c255,c256,c257,c258,c259,c260,
		c261,c262,c263,c264,c265,c266,c267,c268,c269,c270,
		c271,c272,c273,c274,c275,c276,c277,c278,c279,c280,
		c281,c282,c283,c284,c285,c286,c287,c288,c289,c290,
		c291,c292,c293,c294,c295,c296,c297,c298,c299,c300))

##lm(big.reg$ID ~ ed.value)

# plot(big.reg$ID,ed.value,xlab="Item Slope (Rating Value ~ Category Rated)", 
#	ylab="Edwards Scale Value")

paper <- cbind(big.reg,ed.value)

try <- lm(ed.value~ID, paper)

slope <-  as.vector(coef(try)[2]) 

nonmono <- read.csv("nonmono.csv", header=F)	## nonmonotonicity ratings (Kulas, Emily, Brianna)
linear <- read.csv("linearity.csv", header=F)
paper$nonmono <- rowMeans(nonmono, na.rm=TRUE)
paper$linear <- rowMeans(linear, na.rm=TRUE)


#############################################     Some ed.values are missing - 10/12/18 (not seeing missing on 8/12/23)

## New categorical ratings (Emily + Kulas at Dunn Brothers)

newcategories <- read.csv("KuncelTellegen Q Sort Items - Sheet1.csv")[,-c(6,10)]

newcat_long <- tidyr::gather(newcategories, form, item, Linear..positive.slope.:Linear..negative.slope., factor_key=TRUE)

newcat_long_cleaned <- na.omit(newcat_long) ## missing one (n=299)
newcat_sorted <- newcat_long_cleaned[order(newcat_long_cleaned$item),]

paper$category <- newcat_sorted$form


```

```{r lastone, fig.cap="Response Category and Mean Rating slope across Edwards' scale values (individual scatterpoints represent items)", fig.height=5, fig.width=11, warning=FALSE, message=FALSE}

paper$nonmono <- rowMeans(nonmono[,c(2,4)], na.rm=TRUE)   ## Emily + Renata (10/22/19)
paper$linear <- rowMeans(linear[,c(2,4)], na.rm=TRUE)    

## Lots of "1's" for both mono and linear (8/26/23)

paper2 <- paper %>% filter(nonmono > 1)

#b <- ggplot(paper, aes(x=ed.value, y=ID)) +
#   geom_point(aes(size=linear, color=nonmono), alpha=.5) +
#   scale_color_continuous(limits=c(1,5)) +
#   guides(color= guide_legend(), size=guide_legend()) + 
#   scale_size_continuous(limits=c(1,5))

#b + geom_smooth(method=lm, se=F, col="cyan", size=1.5)

#ob <- ggplot(paper, aes(x=ed.value, y=ID)) +
#   geom_point(aes(size=linear, color=nonmono), alpha=.5) +
#   scale_color_continuous(limits=c(1, 3), breaks=seq(1, 3, by=1)) +
#   guides(color= guide_legend(), size=guide_legend()) + 
#   scale_size_continuous(limits=c(1, 3), breaks=seq(1, 3, by=1))
#
#ob + geom_smooth(method=lm, se=TRUE, col="cyan", size=1.5)

forcatplot <- paper             ## 6/13/24 at Dunn Bros with Emily - want to omit "uncategorized"

forcatplot <- forcatplot[which(forcatplot$category != 'Uncategorizable'),]

forcatplot$category <- factor(forcatplot$category, levels = c("Linear..positive.slope.", 
                                                        "Egyptian",
                                                        
                                                        "moderate.hook",
                                                        "extreme.hook", 
                                                        "opposite.hook",   ## moved 6/26
                                                        "Flat", 
                                                        "Witches.hat", 

                                                        "extreme.hook.1",
                                                        "moderate.hump..hook.more.toward.center.",
                                                        "Egyptian.R.",  
                                                        "Linear..negative.slope."))  ## rearrange by "closeness to linearity" at Dunn Bros with Emily 6/13/24

forcatplot$category <- recode(forcatplot$category, "Linear..positive.slope." = "linear (p)",
                          "Egyptian" = "non-mono (s)",
                          "moderate.hook" = "non-mono (hook mp)",
                          "extreme.hook" = "non-mono (hook ep)",
                          "opposite.hook" = "non-mono (j)",
                          "Flat" = "non-mono (flat)",
                          "Witches.hat" = "non-mono (peaked)",
                          "extreme.hook.1" = "non-mono (hook en)",
                          "moderate.hump..hook.more.toward.center." = "non-mono (hook mn)",
                          "Egyptian.R." = "non-mono (z)",
                          "Linear..negative.slope." = "linear (n)")

library(dplyr)

correlations <- forcatplot %>%                         ## one positive corr - double check
  dplyr::group_by(category) %>%
  dplyr::summarise(correlation=cor(ID,ed.value))

wgmeans <- forcatplot %>%                         ## average Edwards value
  dplyr::group_by(category) %>%
  dplyr::summarise(mean=mean(ed.value))

dat_text <- data.frame(
  label= paste("r=",round(correlations$correlation,2)) , 
  category=correlations$category,
  x=c(0,0,0,0,0,0,0,0,0,0,0),
  y=c(-.1,-.07,-.07,-.2,-.1,-.3,-.8,.2,.2,.2,.2))


cats <- ggplot(forcatplot, aes(x=ed.value, y=ID)) + geom_point() + facet_grid(. ~ category) + ylab("function slope") + xlab("Edwards rating value")  +
        theme(axis.title.x=element_blank(),        
            axis.text.x=element_blank(),
            axis.ticks.x=element_blank(),
            legend.position="none") +
            geom_text(data=dat_text, mapping=aes(x=x, y=y, label=label))
#cats

#############################################
#############################################
#############################################


exampleitems <- rbind(i61, i1, i274, i232, i249, i268, i264, i13, i55, i22, i73) ## changed from i11 to i232 and i197 to i249 (6/26/24)

exampleitems$cat <- rep(c("linear(p)", "xtreme hook(p)", "egyptian(p)", "mod hook(p)", "opposite hook/ladle", "witches hat", "flat", "mod hook", "egyptian", "xtreme hook", "linear"), each=5)

exampleitems$cat <- factor(exampleitems$cat, levels = c("linear(p)", 
                                                        "egyptian(p)",
                                                      
                                                        "mod hook(p)",
                                                        "xtreme hook(p)", 
                                                        "opposite hook/ladle",   ## Moved 6/26
                                                        "flat", 
                                                        "witches hat", 
                                                        "xtreme hook",
                                                        "mod hook",
                                                        "egyptian",  
                                                        "linear"))  ## rearrange by "closeness to linearity" at Dunn Bros with Emily 6/13/24
exampleitems$cat <- recode(exampleitems$cat, "linear(p)" = "linear (p)",
                                             "egyptian(p)" = "non-mono (s)",
                                             "mod hook(p)" = "non-mono (hook mp)",
                                             "xtreme hook(p)" = "non-mono (hook ep)",
                                             "opposite hook/ladle" = "non-mono (j)",
                                             "flat" = "non-mono (flat)",
                                             "witches hat" = "non-mono (peaked)",
                                             "xtreme hook" = "non-mono (hook en)",
                                             "mod hook" = "non-mono (hook mn)",
                                             "egyptian" = "non-mono (z)",
                                             "linear" = "linear (n)")

dat_text2 <- data.frame(
  label= paste("m=",round(wgmeans$mean,2)), 
  category=wgmeans$category,
  x=rep("Average",11),
  y=rep(4,11))

addto3 <- ggplot(exampleitems, aes(x=level, y=mean, colour=Edwards, group=1)) + 
         geom_line(linewidth=1) +  
         scale_x_discrete(limits=c("Extremely High", "Above Average", "Average", "Below Average", "Extremely Low")) + 
         facet_wrap(~ cat, nrow=1) +
         ylab("mean rating") +
         theme(axis.title.x=element_blank(),        
            axis.text.x=element_blank(),
            axis.ticks.x=element_blank(),
            legend.position="none") +
#                axis.text.x = element_text(angle=45, vjust=1, hjust=1)) +
          scale_y_continuous(
          labels = scales::number_format(accuracy = 0.1)) 
# + geom_text(data=dat_text2, mapping=aes(x=x, y=y, label=label))

library(ggpubr)

ggarrange(addto3, cats,                     ## switched top and bottom & labels 6/15/24
          ncol=1,
          nrow=2)

############################################# 
#############################################
############################################# Trying categorizations - attempting better viz

paper$strat[paper$ed.value < -2.9 ] <- "-3 or lower"  ## broken (3/23/24) - changed from $ID to $ed.value
paper$strat[paper$ed.value >= -2.9 & paper$ed.value < -1.9 ] <- "-2 to -3"  
paper$strat[paper$ed.value >= -1.9 & paper$ed.value < -.9 ] <- "-1 to -2"  
paper$strat[paper$ed.value >= -.9 & paper$ed.value < .9 ] <- "-1 to 1"  
paper$strat[paper$ed.value >= .9 & paper$ed.value < 1.9 ] <- "1 to 2"  
paper$strat[paper$ed.value >= 1.9 & paper$ed.value < 2.9 ] <- "2 to 3"  
paper$strat[paper$ed.value >= 2.9 ] <- "greater than 3"  

############################################
############################################
############################################ Within category correlations - 6/13/24

```

```{r qsorttable}

dat_text3 <- data.frame(
  label= paste("m=",round(wgmeans$mean,2)), 
  category=wgmeans$category,
  n=c(21,46,16,22,15,11,46,39,17,38,26))


papaja::apa_table(dat_text3, 
                  caption = "Average Edwards rating and number of functional classifications",
                  note = "(p) = positive; (s) = 's' shaped; (hook mp) = positive moderate hook shaped; (hook ep) = positive extreme hook shaped ; (j) = 'j' shaped; (flat) = lays horizonally; (peaked) =  peaks near the middle; (hook en) = negative extreme hook shaped; (hook mn) = negative moderate hook shaped; (z) = 'z' shaped; (n) = negative
",
                  row.names=FALSE,
                  escape = FALSE) 

```

```{r Figure4, fig.cap="Percentage of Kuncel Tellegen Categories that Fall within Edwards Strata", fig.width=8, fig.height=6, echo=FALSE}


ed.high     <- forcatplot[(forcatplot$ed.value <= -3),]
ed.somehigh <- forcatplot[(forcatplot$ed.value > -3) & (forcatplot$ed.value <=-2),]
ed.modhigh  <- forcatplot[(forcatplot$ed.value > -2) & (forcatplot$ed.value <=-1),]
ed.zerohigh     <- forcatplot[(forcatplot$ed.value > -1) & (forcatplot$ed.value < 0),]
ed.zero      <- forcatplot[(forcatplot$ed.value == 0),]
ed.zerolow      <- forcatplot[(forcatplot$ed.value > 0) & (forcatplot$ed.value < 1),]
ed.modlow   <- forcatplot[(forcatplot$ed.value >= 1) & (forcatplot$ed.value < 2),]
ed.somelow  <- forcatplot[(forcatplot$ed.value >= 2) & (forcatplot$ed.value < 3),]
ed.low      <- forcatplot[(forcatplot$ed.value >= 3),]

ed.high$edcat      <- "-3 and Below"
ed.somehigh$edcat  <- "-3 to -2"
ed.modhigh$edcat   <- "-2 to -1"
ed.zerohigh$edcat  <- "-1 to 0"
ed.zero$edcat      <- "0"
ed.zerolow$edcat   <- "0 to 1"
ed.modlow$edcat    <- "1 to 2"
ed.somelow$edcat   <- "2 to 3"
ed.low$edcat       <- "3 and Above"

newtable <- rbind(ed.high,ed.somehigh,ed.modhigh,ed.zerohigh,ed.zero,ed.zerolow,ed.modlow,ed.somelow,ed.low)

newtable$edcat <- factor(newtable$edcat, levels = c("-3 and Below",
                                                    "-3 to -2",
                                                    "-2 to -1",
                                                    "-1 to 0",
                                                    "0",  
                                                    "0 to 1",
                                                    "1 to 2",
                                                    "2 to 3",
                                                    "3 and Above"))

write.csv(table(newtable$edcat,newtable$category),"EdwardsCategories.csv")

library(tidyr)
library(forcats)

mapdata <- read.csv("EdwardsCategories.csv")

##rename colnames so they're easier to read/match other visuals
colnames(mapdata) <- c("edcat", "Linear (p)", "Nonmono (s)", "Nonmono Hook (mp)", "Nonmono Hook (ep)", "Nonmono (j)", "Nonmono Flat", "Nonmono Peaked", "Nonmono Hook (en)", "Nonmono Hook (mn)", "Nonmono (z)", "Linear (n)")

##need long table
#edcat category percent
mapdata <- mapdata %>%
  pivot_longer(cols = `Linear (p)`:`Linear (n)`,
             names_to = "Category",
             values_to = "Percentage")


##HEATMAP Linear (P) Top on Y Axis
##used fct_inorder for x axis; needed to use fct_relvel for y --- I do not understand why it needs to be in reverse order, but it apparently
##has to be in reverse order.
heatmap <- ggplot(data = mapdata, mapping = aes(x = fct_relevel(Category, c( "Linear (p)", "Nonmono (s)", "Nonmono Hook (mp)", "Nonmono Hook (ep)",
                                                                             "Nonmono (j)", "Nonmono Flat", "Nonmono Peaked", "Nonmono Hook (en)",
                                                                             "Nonmono Hook (mn)", "Nonmono (z)", "Linear (n)")),
                                                                              y= fct_inorder(edcat), fill = Percentage)) +
  geom_tile(color = "white",
            lwd = 1.5,
            linetype = 1) +
  scale_fill_gradient(low = "lightblue", high = "black") +
  coord_fixed() +
  xlab(label = "Edwards") +
  ylab(label = "Category")

heatmap + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) 
```

## Study 1 Discussion 

Across approaches, results tended to converge on similar patterns. The preponderance of our results suggest that, in general, linear or "close-to" linear relationships with social desirability (across response options) may fairly well represent the plurarity of assessment item functions. Additionally, similar information may be available *for particular items* through both the traditional and contemporary measurement approaches.

For example, the plurality of the subjective functional form classifications presented in Figure \@ref(fig:lastone) could reasonably be described as "effectively" linear. That is, the contributor(s) to non-linearity occur(s) in the extremes (e.g., "most extreme" being somewhat less desirable than "moderately extreme", however, the general pattern procedes along the Kuncel & Tellegen continuum in a generally progressive fashion). This suggests that an addition to the @kuncel_conceptual_2009 functional form specifications could be an explication of the *location* of the non-monoticity (e.g., linearity is the expectation, deviations are differently meaningful depending on where they manifest -- perhaps they are *not as important* in the extremes (for example, "s" and "z" Figure \@ref(fig:lastone) specifications). 

Although more meaningful deviations from linear functions do exist, they are predominantly associated with *moderately rated* Edwards items. Consultation of the 25 randomly sampled item functions (Figure \@ref(fig:Figure2)) also reflects this pattern of non-monotonicity.[^avail] It is quite plausible that the *most informatively meaningful* "U" or "inverted U" shaped functions, when they occur, are reflective of some ambiguous or contextually primed desirability, and that this ambiguity or contextual moderation results in "middle ground" evaluation via the Edwards method.

[^avail]: A full list of all 300 item functions are available in this paper's online resources. 


# Study 2
One of our observations across the two item rating procedures has been that the @kuncel_conceptual_2009 approach is *more cognitively taxing* for raters than is the @edwards_relationship_1953 rating task. Study Two therefore collected ratings via computer, with estimates of difficulty operationalized as response latencies. 

# Methods 
## Participants

One hundred and thirty one undergraduate students from two universities provided item ratings.  

## Procedure

All ratings were gathered online via @qualtrics_qualtrics_2014, utilizing the Kuncel and Tellegen (2009) rating scale. The ratings included fifteen of the most archetypal items from Study 1, focusing on three functional forms: asymmetric, linear, and quasi-symmetric. Because response latencies were of interest, two practice items were included at the beginning of the procedure so participants would have process familiarity prior to substantive data collection.

# Results

Applying conservative data screens excluded 30 individuals with total survey durations less than 300 seconds (n=16) or total response latencies less than 20 seconds across the final 20 items (on average less than 1 second per item response; n=14). These screens were applied based on observations from experiment proctors that several participants were not fully engaged in their responses. Additionally, two participants self-disclosed that they responded "not at all carefully" and were excluded from analysis. Further carelessness screens included 10 or more consecutive non-differentiating responses (n=7) as well as intra-individual response variability estimates of two standard deviations below the mean (*M*=0.88; n=5) [see, for example, @dunn_intra-individual_2018; @marjanovic_inter-item_2015] via the *careless* R package [@R-careless] in R version `r as.character(getRversion())` [@R-base].

```{r getting study data, include=FALSE}
##STUDY 2 - Interrater Reliabilities (FINAL SCRIPT)

##PACKAGES NEEDED
library(dplyr)
library(irr)
library(psych)
library(careless)
library(splitstackshape)
library(stringr)
library(readr)
#library(naniar)
library(tidyverse)

##Dataset outside of github
raw <- read.csv("15ItemCSV_manipcolheaders.csv")

##NOTE 133 Observations to start with. Need to further clean data. 

##Remove preview responses. Leaves 131 observations.
Cleaned <- raw %>%
  filter(Response.Type != 'Survey Preview')

##Removing where people answered that they basically did not do the task. 129 observations.
Cleaned <- Cleaned %>%
  filter(Q613 != 'Not at All Carefully') 

##removing total duration for those that took too long and too little time (5 minutes).
Cleaned <- Cleaned %>%  
  filter(Duration..in.seconds.>300) 


#look at last 20 items and determine if any should be removed due to giving up.
Cleaned$giveup <- rowSums(Cleaned[c("Timing...Last.Click.84",
                                "Timing...Last.Click.83",
                                "Timing...Last.Click.82",
                                "Timing...Last.Click.81",
                                "Timing...Last.Click.80",
                                "Timing...Last.Click.79",
                                "Timing...Last.Click.78",
                                "Timing...Last.Click.77",
                                "Timing...Last.Click.76",
                                "Timing...Last.Click.75",
                                "Timing...Last.Click.74",
                                "Timing...Last.Click.73",
                                "Timing...Last.Click.72",
                                "Timing...Last.Click.71",
                                "Timing...Last.Click.70",
                                "Timing...Last.Click.69",
                                "Timing...Last.Click.68",
                                "Timing...Last.Click.67",
                                "Timing...Last.Click.66",
                                "Timing...Last.Click.65")], na.rm=TRUE)

#removed 20 - average of 1 second per item for the last 20 items (roughly bottom 10%)
Cleaned <- Cleaned %>%
  filter(giveup>20) 


###CARELESSNESS ITEMS NOTES:

#Q659 = What do you put on your feet?
#Q660 = How many days are in a week?
#Q661 = Which animal is the biggest?
#Q662 = Which of the following animals lives in the ocean?
#Q663 = What state is Montclair State University in?
#Q664 = Which one of the following is the highest degree of education?
#Q665 = Which one of the following animals is smallest?
#Q666 = Which of the following animals is a carnivore?
#Q667 = Which one of the following is NOT a winter sport?
#Q668 = Which of the following is a fruit?
#Q669 = Which of the following do you put on your hands?
#Q670 = The Empire State building is in:
#Q671 = Which is not a social media platform?

careless_items <- Cleaned %>% dplyr::select(c(Q659,Q660,Q661,Q662,Q663,Q664,Q665,Q666,Q667,Q668,
                                       Q669,Q670,Q671))

table(careless_items$Q671)

#Q665 -- my guess is people didn't know what a Sugar Glider was.
#Q670 -- were any of these people from the MN uni?

##decided to keep all due to carelessness items above. There wasn't enough to exclude anyone.
rm(careless_items) ## remove dataframe as it's no longer needed.

##Get row IDs for carelessness indices
Cleaned <- mutate(Cleaned, ID = row_number())

##Carelessness -- parsing down the dataset
RM <- Cleaned %>% dplyr:: select(contains("Desirability"))
ID <- Cleaned %>% dplyr:: select("ID")
RM <- as.data.frame(cbind(RM, ID))

##Removing the first two items as they were not part of the 15
RM <- RM[,11:86]

##Removing no or 1 response -- 11 removed - left with 118
RM[RM == ""] <- NA
RM$NAs <- rowSums(is.na(RM))

RM <- RM %>%
  filter(NAs < 74) 

RM <- as.data.frame(RM$ID)
colnames(RM) <- "ID"

Cleaned <- merge(Cleaned,RM,by="ID", all.x = FALSE, all.y = FALSE) 

rm(RM,ID)

##Now looking at only the items again with 
Only_Items <- Cleaned %>% dplyr::select(contains("Desirability"))

##Only_Items <- Only_Items[!apply(Only_Items == "", 1, all),]
Only_Items[Only_Items == "Very Undesirable"] <-1
Only_Items[Only_Items == "Undesirable"] <-2
Only_Items[Only_Items == "Neutral"] <-3
Only_Items[Only_Items == "Desirable"] <-4
Only_Items[Only_Items == "Very Desirable"] <-5

##Remove first two variables
Only_Items <- Only_Items [,11:85]

##Make the responses numeric instead of string
Only_Items <- as.data.frame(lapply(Only_Items,as.numeric))

##saving a dataframe as wide format
unt_only_items <- Only_Items

##wide to tall dataset
Only_Items <- as.data.frame(t(Only_Items))

##Determine if any have only 1 or 2 responses. Observations = 129 (none removed)
Only_One_Answer <- Only_Items %>% dplyr::select(where(~n_distinct(.) <2)) 
Two_Answers <- Only_Items %>% dplyr::select(where(~n_distinct(.) <3)) 

##Patterns in data (e.g., 1...2...3...4..5...1..2..3..4..5 etc.)
##Start with longest consecutive length of one answer. ##Need to determine how many responses in a row is okay.
Consecutive <- as.data.frame(longstring(unt_only_items, avg=FALSE))
table(Consecutive) ##Quite a few with consecutive responses - what should be the cutoff for this?

##IRV (Intra-Indiavidiual Response Variability) ##Need to determine what an appropriate variability is for this.
IRV <- as.data.frame(irv(unt_only_items, na.rm = TRUE, split = FALSE, num.split = 3)) ##this is a more random approach

##combine the datasets
Investigate <- as.data.frame(cbind(Consecutive, IRV))

##CLEANED DATA
##Participants to Remove + Why
##Participants that have >10 consecutive answers will be removed.
Data <- (cbind(Cleaned, Investigate))

##Removed 8 values from the Long String Index (below)
Data  <-Data %>%
 filter(`longstring(unt_only_items, avg = FALSE)` <= 10) 

##Mean after consecutive items = 1.22 ---> SD = 0.17 ---> 1 SDs below mean = 1.05 --> 2 SDs below = 0.88
describe <- describe(Data$`irv(unt_only_items, na.rm = TRUE, split = FALSE, num.split = 3)`)


##This removed 2 participants for IRV; determined by the standard deviation
Data <-Data %>%
  filter(`irv(unt_only_items, na.rm = TRUE, split = FALSE, num.split = 3)` >= 0.88) 

##Just grabbing ID to match onto dataset
Data <- as.data.frame(Data$ID)
colnames(Data) <- "ID"


##Official Cleaned Data
Cleaned <- inner_join(Cleaned,Data, by = "ID")
Cleaned <- as.data.frame(Cleaned[,18:459])

##Removing unneeded dataframes
rm(Data,Investigate,IRV,Only_Items, Only_One_Answer,Two_Answers,unt_only_items,Consecutive, describe)


#latencies
Latency <- Cleaned %>% dplyr:: select(contains("Last.Click"))
Latency <- Latency[,11:85] ##removing test items
colSums(is.na(Latency))

##if 0 then NA
Latency [Latency == 0.000] <- NA
colSums(is.na(Latency))

##get bottom and top values @ 5% to remove from Latency data frame
Lat_unlist <- Latency

Lat_unlist <- data.frame(x=unlist(Lat_unlist))
Lat_unlist <- Lat_unlist %>% drop_na()

quantile(Lat_unlist$x, 0.05) ##anything below 0.63
quantile(Lat_unlist$x, 0.95) #anything above 12.5544

##NA from bottom and top 5% from above

Latency <- pmin(Latency, 12.5544)
Latency [Latency == 12.5544] <- NA
colSums(is.na(Latency))

###

Latency <-pmax(Latency, 0.63)
Latency [Latency == 0.63] <- NA
colSums(is.na(Latency))

min(Latency, na.rm = TRUE)
max(Latency, na.rm = TRUE)

##Latency Descriptives##

des <- describe(Latency)

#MEAN
means <- as.data.frame(des$mean)

means$level <- NA 
level <- c("EH", "AA", "A", "BA", "EL")
means$level <- rep(level, each = 1)

means$type <- NA
type <- c("asym","linear","asym","linear", "linear", "asym","quasi", "asym","asym", "linear","quasi","linear","quasi","quasi","quasi")
means$type <- rep(type, each=5) 

means$item <- NA
item <- c("Worry", "Friends","Trust", "Angry", "Blue", "Charge", "Busy", "Sat", "Excite", "Symp","fant","irr","go","relax","habit")
means$item <- rep(item, each=5) 

means$neg <- NA
neg <- c("Y", "N", "N", "Y", "Y", "N","N","N","N","N","N","Y","N","N","N")
means$neg <- rep(neg, each=5) 

rm(item,level,neg,type)


#Latency Mean by Level
level_avg <- means %>% 
  group_by(level) %>% 
  summarise(mean_level = mean(`des$mean`, na.rm = TRUE))

type_avg <- means %>% 
  group_by(type) %>% 
  summarise(mean_type = mean(`des$mean`, na.rm = TRUE))

item_avg <- means %>% 
  group_by(item) %>% 
  summarise(mean_item = mean(`des$mean`, na.rm = TRUE))

neg_avg <- means %>% 
  group_by(neg) %>% 
  summarise(mean_neg = mean(`des$mean`, na.rm = TRUE))

rm(des,Lat_unlist,means,level_avg,neg_avg,type_avg)


##Going Back to Only Items in the dataset
Cleaned <- Cleaned %>% dplyr:: select(contains("Desirability"))
Cleaned[Cleaned == "Very Undesirable"] <-1
Cleaned[Cleaned == "Undesirable"] <-2
Cleaned[Cleaned == "Neutral"] <-3
Cleaned[Cleaned == "Desirable"] <-4
Cleaned[Cleaned == "Very Desirable"] <-5
Cleaned[Cleaned == ""] <-NA

Cleaned <- as.data.frame(lapply(Cleaned,as.numeric))


#### INTERRATER RELIABILITY ###
library(irr)

##WORRY
Worry <- Cleaned[,11:15]
Worry <- t(Worry)
Worry <- Worry[ , colSums(is.na(Worry)) == 0]

##Run ICC 
Worry_f <- icc(Worry, model = "twoway", type = "agreement")
Worry_f <- Worry_f$value

rm(Worry)



##FRIENDS
Friends <- Cleaned[,16:20]
Friends <- t(Friends)
Friends <- Friends[ , colSums(is.na(Friends)) == 0]

##Run ICC 
Friends_f <- icc(Friends,model = "twoway", type = "agreement")
Friends_f <- Friends_f$value

rm(Friends)

##TRUST
Trust <- Cleaned[,21:25]
Trust <- t(Trust)
Trust <- Trust[ , colSums(is.na(Trust)) == 0]

##Run ICC 
Trust_f <- icc(Trust, model = "twoway", type = "agreement")
Trust_f <- Trust_f$value

rm(Trust)

##ANGRY
Angry <- Cleaned[,26:30]
Angry <- t(Angry)
Angry <- Angry[ , colSums(is.na(Angry)) == 0]

##Run ICC 
Angry_f <- icc(Angry, model = "twoway", type = "agreement")
Angry_f <- Angry_f$value

rm(Angry)

##BLUE
Blue <- Cleaned[,31:35]
Blue <- t(Blue)
Blue <- Blue[ , colSums(is.na(Blue)) == 0]

##Run ICC 
Blue_f <- icc(Blue, model = "twoway", type = "agreement")
Blue_f <- Blue_f$value

rm(Blue)

##TAKE CHARGE
Charge <- Cleaned[,36:40]
Charge <- t(Charge)
Charge <- Charge[ , colSums(is.na(Charge)) == 0]

##Run ICC 
Charge_f <- icc(Charge, model = "twoway", type = "agreement")
Charge_f <- Charge_f$value

rm(Charge)

##AM BUSY
Busy <- Cleaned[,41:45]
Busy <- t(Busy)
Busy <- Busy[ , colSums(is.na(Busy)) == 0]

##Run ICC 
Busy_f <- icc(Busy, model = "twoway", type = "agreement")
Busy_f <- Busy_f$value

rm(Busy)

##SATISFY
Sat <- Cleaned[,46:50]
Sat <- t(Sat)
Sat <- Sat[ , colSums(is.na(Sat)) == 0]

##Run ICC 
Sat_f <- icc(Sat, model = "twoway", type = "agreement")
Sat_f <- Sat_f$value

rm(Sat)

##EXCITE
Excite <- Cleaned[,51:55]
Excite <- t(Excite)
Excite <- Excite[ , colSums(is.na(Excite)) == 0]

##Run ICC 
Excite_f <- icc(Excite, model = "twoway", type = "agreement")
Excite_f <- Excite_f$value

rm(Excite)

##SYMPATHY
Symp <- Cleaned[,56:60]
Symp <- t(Symp)
Symp <- Symp[ , colSums(is.na(Symp)) == 0]

##Run ICC 
Symp_f <- icc(Symp, model = "twoway", type = "agreement")
Symp_f <- Symp_f$value

rm(Symp)

##FANTASY
fant <- Cleaned[,61:65]
fant <- t(fant)
fant <- fant[ , colSums(is.na(fant)) == 0]

##Run ICC 
fant_f <- icc(fant, model = "twoway", type = "agreement")
fant_f <- fant_f$value

rm(fant)

##IRRITATED
irr <- Cleaned[,66:70]
irr <- t(irr)
irr <- irr[ , colSums(is.na(irr)) == 0]

##Run ICC 
irr_f <- icc(irr, model = "twoway", type = "agreement")
irr_f <- irr_f$value

rm(irr)

##ON THE GO
go <- Cleaned[,71:75]
go <- t(go)
go <- go[ , colSums(is.na(go)) == 0]

##Run ICC 
go_f <- icc(go, model = "twoway", type = "agreement")
go_f <- go_f$value

rm(go)

##RELAX
relax <- Cleaned[,76:80]
relax <- t(relax)
relax <- relax[ , colSums(is.na(relax)) == 0]

##Run ICC 
relax_f <- icc(relax, model = "twoway", type = "agreement")
relax_f <- relax_f$value

rm(relax)

##HABITS
habit <- Cleaned[,81:85]
habit <- t(habit)
habit <- habit[ , colSums(is.na(habit)) == 0]

##Run ICC 
habit_f <- icc(habit, model = "twoway", type = "agreement")
habit_f <- habit_f$value

rm(habit)

##ALL
all <- Cleaned[,11:85]
all <- t(all)
all <- all[ , colSums(is.na(all)) == 0]

##Run ICC 
all_f <- icc(all, model = "twoway", type = "agreement")
all_f <- all_f$value

rm(all)

###MERGE

ICC_all <- as.data.frame(cbind(Worry_f, Trust_f, Charge_f, Sat_f, Excite_f, Friends_f, Angry_f, Blue_f,Symp_f, irr_f, Busy_f, fant_f, go_f, relax_f,habit_f,all_f))

ICC_all$Asymmetric.mean <- rowMeans(ICC_all[,1:5]) ##Asymmetric mean = 0.263
ICC_all$Linear.mean <- rowMeans(ICC_all[,6:10]) ##Linear mean = 0.227
ICC_all$Quasi.mean <- rowMeans(ICC_all[,11:15]) ##Quasi mean = 0.13

ICC_all <- round(ICC_all, digits=3)

rm(all_f,Angry_f,Blue_f,Busy_f,Charge_f,Excite_f,fant_f,Friends_f,go_f,habit_f,irr_f,relax_f,Sat_f,Symp_f,Trust_f,Worry_f)

#write_csv(Kripp_alpha, "E:/SD Research/Social Desirability/Krippalpha.csv")


##Grab Kripp from Study 1 (Confirmed Correct Data)
studyone <- read.csv("Study1CompareData.csv")


##WORRY
Worry <- studyone[,27:31]
Worry <- t(Worry)
Worry <- Worry[ , colSums(is.na(Worry)) == 0]

##Run ICC 
Worry_f <- icc(Worry, model = "twoway", type = "agreement")
Worry_f <- Worry_f$value

rm(Worry)



##FRIENDS
Friends <- studyone[2:6]
Friends <- t(Friends)
Friends <- Friends[ , colSums(is.na(Friends)) == 0]

##Run ICC 
Friends_f <- icc(Friends, model = "twoway", type = "agreement")
Friends_f <- Friends_f$value

rm(Friends)

##TRUST
Trust <- studyone[,32:36]
Trust <- t(Trust)
Trust <- Trust[ , colSums(is.na(Trust)) == 0]

##Run ICC 
Trust_f <- icc(Trust, model = "twoway", type = "agreement")
Trust_f <- Trust_f$value

rm(Trust)

##ANGRY
Angry <- studyone[,7:11]
Angry <- t(Angry)
Angry <- Angry[ , colSums(is.na(Angry)) == 0]

##Run ICC 
Angry_f <- icc(Angry, model = "twoway", type = "agreement")
Angry_f <- Angry_f$value

rm(Angry)

##BLUE
Blue <- studyone[,12:16]
Blue <- t(Blue)
Blue <- Blue[ , colSums(is.na(Blue)) == 0]

##Run ICC 
Blue_f <- icc(Blue, model = "twoway", type = "agreement")
Blue_f <- Blue_f$value

rm(Blue)

##TAKE CHARGE
Charge <- studyone[,37:41]
Charge <- t(Charge)
Charge <- Charge[ , colSums(is.na(Charge)) == 0]

##Run ICC 
Charge_f <- icc(Charge, model = "twoway", type = "agreement")
Charge_f <- Charge_f$value

rm(Charge)

##AM BUSY
Busy <- studyone[,52:56]
Busy <- t(Busy)
Busy <- Busy[ , colSums(is.na(Busy)) == 0]

##Run ICC 
Busy_f <- icc(Busy, model = "twoway", type = "agreement")
Busy_f <- Busy_f$value

rm(Busy)

##SATISFY
Sat <- studyone[,44:46]
Sat <- t(Sat)
Sat <- Sat[ , colSums(is.na(Sat)) == 0]

##Run ICC 
Sat_f <- icc(Sat, model = "twoway", type = "agreement")
Sat_f <- Sat_f$value

rm(Sat)

##EXCITE
Excite <- studyone[,47:51]
Excite <- t(Excite)
Excite <- Excite[ , colSums(is.na(Excite)) == 0]

##Run ICC 
Excite_f <- icc(Excite, model = "twoway", type = "agreement")
Excite_f <- Excite_f$value

rm(Excite)

##SYMPATHY
Symp <- studyone[,17:21]
Symp <- t(Symp)
Symp <- Symp[ , colSums(is.na(Symp)) == 0]

##Run ICC 
Symp_f <- icc(Symp, model = "twoway", type = "agreement")
Symp_f <- Symp_f$value

rm(Symp)

##FANTASY
fant <- studyone[,57:61]
fant <- t(fant)
fant <- fant[ , colSums(is.na(fant)) == 0]

##Run ICC 
fant_f <- icc(fant, model = "twoway", type = "agreement")
fant_f <- fant_f$value

rm(fant)

##IRRITATED
irr <- studyone[,22:26]
irr <- t(irr)
irr <- irr[ , colSums(is.na(irr)) == 0]

##Run ICC 
irr_f <- icc(irr, model = "twoway", type = "agreement")
irr_f <- irr_f$value

rm(irr)

##ON THE GO
go <- studyone[,62:66]
go <- t(go)
go <- go[ , colSums(is.na(go)) == 0]

##Run ICC 
go_f <- icc(go, model = "twoway", type = "agreement")
go_f <- go_f$value

rm(go)

##RELAX
relax <- studyone[,67:71]
relax <- t(relax)
relax <- relax[ , colSums(is.na(relax)) == 0]

##Run ICC 
relax_f <- icc(relax, model = "twoway", type = "agreement")
relax_f <- relax_f$value

rm(relax)

##HABITS
habit <- studyone[,72:76]
habit <- t(habit)
habit <- habit[ , colSums(is.na(habit)) == 0]

##Run ICC 
habit_f <- icc(habit, model = "twoway", type = "agreement")
habit_f <- habit_f$value

rm(habit)

##ALL
all <- studyone[,2:76]
all <- t(all)
all <- all[ , colSums(is.na(all)) == 0]

##Run ICC 
all_f <- icc(all, model = "twoway", type = "agreement")
all_f <- all_f$value

rm(all)

###MERGE

studyone_all <- as.data.frame(cbind(Worry_f, Trust_f, Charge_f, Sat_f, Excite_f, Friends_f, Angry_f, Blue_f,Symp_f, irr_f, Busy_f, fant_f, go_f, relax_f,habit_f,all_f))

studyone_all$Asymmetric.mean <- rowMeans(studyone_all[,1:5]) 
studyone_all$Linear.mean <- rowMeans(studyone_all[,6:10]) 
studyone_all$Quasi.mean <- rowMeans(studyone_all[,11:15]) 

studyone_all <- round(studyone_all, digits=3)

rm(all_f,Angry_f,Blue_f,Busy_f,Charge_f,Excite_f,fant_f,Friends_f,go_f,habit_f,irr_f,relax_f,Sat_f,Symp_f,Trust_f,Worry_f)

##STUDY 1 EDWARDS ICC

edwards <- read.csv("Edwards_15_Isolated_reduced.csv")


des <- describe(edwards)

linear <- edwards[,2:6]
tlinear<- t(linear)
lineara <- tlinear[,colSums(is.na(tlinear))<nrow(tlinear)]
##Run ICC 
lineara_f <- icc(lineara, model = "twoway", type = "agreement")
lineara_f <- lineara_f$value

quasi <- edwards[,7:11]
tquasi<- t(quasi)
quasia <- tquasi[,colSums(is.na(tquasi))<nrow(tquasi)]
##Run ICC 
quasia_f <- icc(quasia, model = "twoway", type = "agreement")
quasia_f <- quasia_f $value


asym <- edwards[,12:16]
tasym<- t(asym)
asyma <- tasym[,colSums(is.na(tasym))<nrow(tasym)]
##Run ICC 
asyma_f <- icc(asyma, model = "twoway", type = "agreement")
asyma_f <- asyma_f$value

all <- edwards[,2:16]
tall<- t(all)
all<- tall[,colSums(is.na(tall))<nrow(tall)]
##Run ICC 
all_f <- icc(all, model = "twoway", type = "agreement")
all_f <- all_f$value


edwards_alpha <- as.data.frame(cbind(asyma_f,lineara_f,quasia_f,all_f))
rm(lineara,quasia,asyma)

edwards_alpha <- edwards_alpha %>% 
  mutate_if(is.numeric, round, digits=3)

rm(asyma_f,tasym,tlinear,tquasi,lineara_f,quasia_f,linear,quasi,des,asym,all_f,tall,all)


###MERGE


##COMBINE ICC from STUDY 1 and 2 

All_ICC <- as.data.frame(rbind(ICC_all,studyone_all))

tALL_ICC <- as.data.frame(t(All_ICC))

rownames(tALL_ICC) <- c("Worry about things","Trust others","Take charge","Am easy to satisfy", "Love excitement", "Make friends easily", "Get angry easily", "Often feel blue", "Sympathize with the homeless", "Get irritated easily", "Am always busy", "Enjoy wild flights of fantasy", "Am always on the go", "Am relaxed most of the time", "Am a creature of habit", "All items", "Asymmetric Mean", "Linear Mean", "Quasi Mean") 

colnames(tALL_ICC) <- c("Study 2", "Study 1")

#tALL_ICC <- as.data.frame(cbind(tALL_ICC[,2], tALL_ICC[,1]))


##GET ITEM GRAPHS && OVERLAY KRIPP VALUES?? (Look at those categories from the 
##paper groupings too to see if there is anything there)


#install.packages("papaja")
#library(papaja)
#papaja::apa_table(tALL_ICC)
```

### Response Latencies
 
  All latencies were taken and compiled into one list so that outliers could be removed. Any latencies that were 0 were first removed, then any time below the 10th percentile and above 90th percentile, from the remaining times, were removed. Afterwards, mean latencies were gathered for four conditions: the item (e.g., worry about things), the level (e.g., extremely high), the type (e.g., linear), and negatively worded items.  
  
  Latency data had some differences in type with quasisymmetric taking the least amount of time (*M* = 2.8 seconds) and asymmetric taking the most time (*M* = 3.5 seconds). Negatively worded items took on average 0.5s longer than non-negatively worded items. The extremely high level had the largest latency time at 6.0 seconds, while the other levels ranged from 2.0s to 3.0s. However, this is likely an artifact due to extremely high levels being the first rating after reading the item characteristic, rather than it being harder to rate. 
  
### Interrater Reliability

ICC(2,1) was used for interrater reliability (BIB REF) for Study 1 and Study 2 comparisons on the Edwards (1957b) as well as Kuncel and Tellegen (2009) methodologies. For Edwards, the ICC(2,1) was run for the three functional forms, and then all ratings were combined to obtain an overall ICC. However, for the Kuncel and Tellgen method, interrater reliability was first run on the item itself, and then all interrater reliabilities were averaged to obtain the three functional form types for Study 1 and Study 2. Kuncel and Tellegen method ICCs are shown in Table 3.

The Edwards interrater reliability was moderate for linear functional forms (ICC = 0.525), and had no interrater reliability for asymmetric (ICC = 0.001) or quasisymmetric (ICC = 0.000) items. Overall reliability of all 15 items was poor (ICC = 0.277). The Kuncel and Tellegen functional form interrater reliability for study 1 was moderate for linear (ICC = 0.503), and poor  for asymmetric (ICC = 0.445) and quasisymmetric (ICC = 0.099). Study 2, however, had poor interrater reliability for all function forms: linear (ICC = 0.324), asymmetric (ICC = 0.303), quasisymmetric (ICC = 0.141). 


```{r Table 3}

papaja::apa_table(tALL_ICC, caption = "Individual ICC",row.names=TRUE)

```

Averaged ICC by functional form and condition are presented in Figure 4 via bars. In addition, the Kuncel Tellegen conditions include error terms related to the items in their category with the lowest and highest ICCs. Error terms were highest for asymmetric forms in both Study 1 (error bar span = 0.41) and Study 2 (0.44), with linear in both studies, and quasisymmetric in study 2 were in the 0.30s range (study 1 linear: 0.31, study 2 linear: 0.35, study 2 quasi: 0.36). The lowest error was the quasisymmetric form in study 1 at 0.18 units. 

```{r,echo=FALSE, include = FALSE}

ind_icc <- tALL_ICC[17:19,]
level <- c("Asymmetric","Linear", "Quasisymmetric")
ind_icc$level <- rep(level, each=1) 

study1 <- ind_icc[,2:3]
study1$study <- "Study 1"
names(study1)[names(study1) == 'Study 1'] <- 'value'

study2 <- subset(ind_icc, select=c("Study 2","level"))
study2$study <- "Study 2"
names(study2)[names(study2) == 'Study 2'] <- 'value'

tedwards <- t(edwards_alpha)
tedwards <- as.data.frame(tedwards[1:3,])
names(tedwards)[names(tedwards) == 'tedwards[1:3, ]'] <- 'value'
tedwards$level <- rep(level, each=1) 
tedwards$study <- "Edwards"

ind_icc <- rbind(tedwards,study1,study2)

describe(studyone_all)
describe(ICC_all)

#edwards, study 1, study 2 - asym, linear, quasi
#edwards mins and maxes are dummy data to hide the error bars - I don't know how to exclude them otherwise.

ind_icc$min <- c(0.005,0.05,-0.001,0.21,0.35,0.00,0.055,0.191,0.012)
ind_icc$max <- c(0.005,0.05,-0.001,0.62,0.66,0.18,0.490,0.540,0.373)

##Needed to make value a numeric to get the study values to go in order of study rather than by low to high
ind_icc$value <- as.numeric(ind_icc$value)
str(ind_icc)

p <- ggplot(data=ind_icc, aes(x=level, y=value, fill=study)) + 
geom_bar(stat="identity", position=position_dodge())  + 
  scale_fill_manual(values=c("black", "darkgray", "lightgray")) +
  geom_errorbar(aes(ymin = min, ymax = max), width = .2, position=position_dodge(.9))

```

```{r Figure5,echo=FALSE,fig.cap=" Mean ICC by Item Type", fig.width=8, fig.height=6}
p + labs(x="Type", y="Mean ICC", fill = "Study")
```

### Functional Forms 
Figures 5 and 6 represent individual ICCs with their functional forms in the Kuncel and Tellegen conditions for both study 2 and study 1 for comparison. Visually, linear functional forms stayed consistent between study 1 and study 2. Concerning asymmetric forms, study 1 had more extreme visuals in comparison to study 2, going from ep to mp (but these would still be classified as asymmetric forms). Quasisymmetric forms had two items that visually switched functional forms: "Enjoy Wild Flights of Fantasy" (to linear), and "Relaxed Most of the Time" (to asymmetric). 



```{r, Graph Comparison_Study 2 Code, include=FALSE, echo = FALSE, results ='hide', fig.show='hide',warning=FALSE}
###GRAPHS
##install.packages("matrixStats")
library(ggplot2)
library(matrixStats)
library(tidyverse)

##Make Friends; Get Angry; Feel blue; homeless; irritated


linear <- as.data.frame((c((Cleaned[,16:20]), (Cleaned[,26:30]),
                           (Cleaned[,31:35]), (Cleaned[,56:60]),
                           (Cleaned[,66:70]))))

####worry about things, trust others, take charge, am easy to satisfy, love excitement

##worry about things AA and EH needed to switch position - the first three parsers are for worry about things

worry <- as.data.frame(cbind(Cleaned[,12],Cleaned[,11]))

asym <- as.data.frame((c((worry[,1:2]), (Cleaned[13:15]), (Cleaned[,21:25]), 
                         (Cleaned[,36:40]), (Cleaned[,46:50]), (Cleaned[,51:55]))))

##am always busy, enjoy wild flights of fantasy, am always on the go, am relaxed most of the time, am a creature of habit
quasi <- as.data.frame((c((Cleaned[,41:45]), (Cleaned[,61:65]),
                          (Cleaned[,71:75]), (Cleaned[,76:80]),
                          (Cleaned[,81:85])))) 

linear[linear == ""] <- NA
linear$NAs <- rowSums(is.na(linear))
linear <- linear %>%
  filter(NAs < 25)
linear <- linear[,1:25]

linear_des <- describe(linear)
linear <- as.data.frame(t(linear))


asym[asym == ""] <- NA
asym$NAs <- rowSums(is.na(asym))
asym <- asym %>%
  filter(NAs < 25)
asym <- asym[,1:25]
asym_des <- describe(asym)
asym <- as.data.frame(t(asym))


quasi[quasi == ""] <- NA
quasi$NAs <- rowSums(is.na(quasi))
quasi <- quasi %>%
  filter(NAs < 25) 
quasi <- quasi[,1:25]
quasi_des <- describe(quasi)
quasi <- as.data.frame(t(quasi))


##to use in a plot
linear$level <- NA 
linear$item <- NA
level <- c("EH", "AA", "A", "BA", "EL")
item <- c("Make Friends Easily", "Get Angry Easily", "Often Feel Blue", "Sympathize with Homeless", "Get Irritated Easily")
linear$level <- rep(level, each = 1)
linear$item <- rep(item, each=5) 

asym$level <- NA 
asym$item <- NA
level <- c("EH", "AA", "A", "BA", "EL")
item <- c("Worry About Things", "Trust Others", "Take Charge", "Am Easy to Satisfy", "Love Excitement")
asym$level <- rep(level, each = 1)
asym$item <- rep(item, each=5)   

quasi$level <- NA 
quasi$item <- NA
level <- c("EH", "AA", "A", "BA", "EL")
item <- c("Am Always Busy", "Enjoy Wild Flights-Fantasy", "Am Always on the Go", "Relaxed Most of the Time", "Creature of Habit")
quasi$level <- rep(level, each = 1)
quasi$item <- rep(item, each=5)   


linear <- cbind(linear, linear_des$mean, linear_des$sd)
asym <- cbind(asym , asym_des$mean, asym_des$sd)
quasi <- cbind(quasi, quasi_des$mean, quasi_des$sd)


str(linear$`linear_des$mean`)

L <- as.data.frame(cbind(ICC_all$Friends_f, ICC_all$Angry_f, ICC_all$Blue_f, ICC_all$Symp_f, ICC_all$irr_f))

Ldat_text <- data.frame(cbind(item = unique(linear$item),
  label= paste("ICC = ",format(round(L,digits=3),nsmall=3))))


L_plot <- ggplot(linear, aes(x=level, y=`linear_des$mean`, group=1)) + geom_line() +
  scale_x_discrete(limits=c("EH", "AA", "A", "BA", "EL")) +
  facet_wrap(~ item, nrow = 1) +
  scale_colour_brewer() + theme_dark() + theme(axis.text.x = element_text(angle=60, hjust=1, size = 9), axis.title.x = element_blank(), 
  strip.text.x = element_text(size=7, face = "bold")) +
  ylim(1,5) +
  ylab("Linear") +
  geom_text(data= Ldat_text, aes(x=4.2, y=4.5, label=label), size = 3)

L_plot 

A <- as.data.frame(cbind(ICC_all$Worry_f, ICC_all$Trust_f, ICC_all$Charge_f, ICC_all$Sat_f, ICC_all$Excite_f))

Adat_text <- data.frame(cbind(item = unique(asym$item),
  label= paste("ICC = ",format(round(A,digits=3),nsmall=3))))

A_plot <- ggplot(asym, aes(x=level, y=`asym_des$mean`, group=1)) + geom_line() +
  scale_x_discrete(limits=c("EH", "AA", "A", "BA", "EL")) +
  facet_wrap(~ item, nrow = 1) +
 scale_colour_brewer() + theme_dark() + theme(axis.text.x = element_text(angle=60, hjust=1, size = 9), axis.title.x = element_blank(),
                                               strip.text.x = element_text(size=7, face = "bold")) +
  ylim(1,5) +
  ylab("Asymmetric") +
  xlab("Social Desirability Level") +
  geom_text(data= Adat_text, aes(x=4.2, y=4.5, label=label), size = 3)

A_plot


Q <- as.data.frame(cbind(ICC_all$Busy_f, ICC_all$fant_f, ICC_all$go_f, ICC_all$relax_f, ICC_all$habit_f))

Qdat_text <- data.frame(cbind(item = unique(quasi$item),
  label= paste("ICC = ",format(round(Q,digits=3),nsmall=3))))

Q_plot <- ggplot(quasi, aes(x=level, y=`quasi_des$mean`, group=1)) + geom_line() +
  scale_x_discrete(limits=c("EH", "AA", "A", "BA", "EL")) +
  facet_wrap(~ item, nrow = 1) +
 scale_colour_brewer() + theme_dark() + theme(axis.text.x = element_text(angle=60, hjust=1, size = 9), axis.title.x = element_blank(),
                                               strip.text.x = element_text(size=7, face = "bold")) +
  ylim(1,5) +
  ylab("Quasisymmetric") +
  geom_text(data= Qdat_text, aes(x=4.2, y=4.5, label=label), size = 3)

Q_plot

##install.packages("gridExtra")
library(gridExtra)

study2plots <- grid.arrange(L_plot,Q_plot, A_plot)


##use code right of ":" to reset if graphic error comes up : dev.off()

rm(asym,asym_des,linear,linear_des,quasi,quasi_des,Qdat_text,Adat_text,Ldat_text,worry,L,A,Q)



```


```{r Figure7, fig.cap="Study 2: Functional Form and ICC by Item", echo=FALSE, fig.width=9, fig.height=6}
study2plots <- grid.arrange(A_plot,L_plot,Q_plot)

rm(A_plot,L_plot,Q_plot)
```



```{r Grpahs Study 1, echo = FALSE, include = FALSE, fig.show='hide'}
###GRAPHS

##Make Friends; Get Angry; Feel blue; homeless; irritated
linear <- as.data.frame((c((studyone[,2:6]), (studyone[,7:11]),
                           (studyone[,12:16]), (studyone[,17:21]),
                           (studyone[,22:26]))))

####worry about things, trust others, take charge, am easy to satisfy, love excitement

##worry about things AA and EH need to switch position 
asym <- as.data.frame((c((studyone[,27:31]), (studyone[,32:36]),
                      (studyone[,37:41]), (studyone[,42:46]),
                      (studyone[,47:51]))))

##am always busy, enjoy wild flights of fantasy, am always on the go, am relaxed most of the time, am a creature of habit
quasi <- as.data.frame((c((studyone[,52:56]), (studyone[,57:61]),
                          (studyone[,62:66]), (studyone[,67:71]),
                          (studyone[,71:76])))) 

linear[linear == ""] <- NA
linear$NAs <- rowSums(is.na(linear))
linear <- linear %>%
  filter(NAs < 25)
linear <- linear[,1:25]

linear_des <- describe(linear)
linear <- as.data.frame(t(linear))


asym[asym == ""] <- NA
asym$NAs <- rowSums(is.na(asym))
asym <- asym %>%
  filter(NAs < 25)
asym <- asym[,1:25]
asym_des <- describe(asym)
asym <- as.data.frame(t(asym))


quasi[quasi == ""] <- NA
quasi$NAs <- rowSums(is.na(quasi))
quasi <- quasi %>%
  filter(NAs < 25) 
quasi <- quasi[,1:25]
quasi_des <- describe(quasi)
quasi <- as.data.frame(t(quasi))


##to use in a plot
linear$level <- NA 
linear$item <- NA
level <- c("EH", "AA", "A", "BA", "EL")
item <- c("Make Friends Easily", "Get Angry Easily", "Often Feel Blue", "Sympathize with Homeless", "Get Irritated Easily")
linear$level <- rep(level, each = 1)
linear$item <- rep(item, each=5) 

asym$level <- NA 
asym$item <- NA
level <- c("EH", "AA", "A", "BA", "EL")
item <- c("Worry About Things", "Trust Others", "Take Charge", "Am Easy to Satisfy", "Love Excitement")
asym$level <- rep(level, each = 1)
asym$item <- rep(item, each=5)   

quasi$level <- NA 
quasi$item <- NA
level <- c("EH", "AA", "A", "BA", "EL")
item <- c("Am Always Busy", "Enjoy Wild Flights-Fantasy", "Am Always on the Go", "Relaxed Most of the Time", "Creature of Habit")
quasi$level <- rep(level, each = 1)
quasi$item <- rep(item, each=5)   


linear <- cbind(linear, linear_des$mean, linear_des$sd)
asym <- cbind(asym , asym_des$mean, asym_des$sd)
quasi <- cbind(quasi, quasi_des$mean, quasi_des$sd)

#library(ggplot2)
str(linear$`linear_des$mean`)

L <- as.data.frame(cbind(studyone_all$Friends_f, studyone_all$Angry_f, studyone_all$Blue_f, studyone_all$Symp_f, studyone_all$irr_f))

Ldat_text <- data.frame(cbind(item = unique(linear$item),
  label= paste("ICC = ",format(round(L,digits=3),nsmall=3))))


L_plot <- ggplot(linear, aes(x=level, y=`linear_des$mean`, group=1)) + geom_line() +
  scale_x_discrete(limits=c("EH", "AA", "A", "BA", "EL")) +
  facet_wrap(~ item, nrow = 1) +
  scale_colour_brewer() + theme_dark() + theme(axis.text.x = element_text(angle=60, hjust=1, size = 9), axis.title.x = element_blank(),
                                               strip.text.x = element_text(size=7, face = "bold")) +
  ylim(1,5) +
  ylab("Linear") +
  geom_text(data= Ldat_text, aes(x=4.2, y=4.5, label=label), size = 3)

L_plot 

A <- as.data.frame(cbind(studyone_all$Worry_f, studyone_all$Trust_f, studyone_all$Charge_f, studyone_all$Sat_f, studyone_all$Excite_f))

Adat_text <- data.frame(cbind(item = unique(asym$item),
  label= paste("ICC = ",format(round(A,digits=3),nsmall=3))))

A_plot <- ggplot(asym, aes(x=level, y=`asym_des$mean`, group=1)) + geom_line() +
  scale_x_discrete(limits=c("EH", "AA", "A", "BA", "EL")) +
  facet_wrap(~ item, nrow = 1) +
 scale_colour_brewer() + theme_dark() + theme(axis.text.x = element_text(angle=60, hjust=1, size = 9), axis.title.x = element_blank(),
                                               strip.text.x = element_text(size=7, face = "bold")) +
  ylim(1,5) +
  ylab("Asymmetric") +
  xlab("Social Desirability Level") +
  geom_text(data= Adat_text, aes(x=4.2, y=4.5, label=label), size = 3)

A_plot


Q <- as.data.frame(cbind(studyone_all$Busy_f, studyone_all$fant_f, studyone_all$go_f, studyone_all$relax_f, studyone_all$habit_f))

Qdat_text <- data.frame(cbind(item = unique(quasi$item),
  label= paste("ICC = ",format(round(Q,digits=3),nsmall=3))))

Q_plot <- ggplot(quasi, aes(x=level, y=`quasi_des$mean`, group=1)) + geom_line() +
  scale_x_discrete(limits=c("EH", "AA", "A", "BA", "EL")) +
  facet_wrap(~ item, nrow = 1) +
 scale_colour_brewer() + theme_dark() + theme(axis.text.x = element_text(angle=60, hjust=1, size = 9), axis.title.x = element_blank(),
                                               strip.text.x = element_text(size=7, face = "bold")) +
  ylim(1,5) +
  ylab("Quasisymmetric") +
  geom_text(data= Qdat_text, aes(x=4.2, y=4.5, label=label), size = 3)

Q_plot

##use code right of ":" to reset if graphic error comes up : dev.off()

rm(asym,asym_des,linear,linear_des,quasi,quasi_des,Qdat_text,Adat_text,Ldat_text,worry,L,A,Q,item, level)
```



```{r Figure6, fig.cap="Study 1: Functional Form and ICC by Item", echo=FALSE, fig.width=9, fig.height=6}
#study1 graph
study1plots <- grid.arrange(A_plot,L_plot,Q_plot)

rm(A_plot,L_plot,Q_plot)
```




# Discussion

>Note. Anecdotally the functions appeared more pronounced on one side versus the other - look at Figure 3 and some "hooks" tended to be stronger (more gentle slopes with the other side)

The plurality of findings do support similar information being conveyed through both approaches. Figure 1 captures *some* of this, as functional slopes (even if somewhat nonmonotonic - see, for example, "Seldom Daydream" in Figure 1) tend to be more extreme with Edwards' highly desirable or undesirable items, and more flat with Edwards' moderate items.[^4] Table 1 presents the frequency with which researcher-implicated functional shapes (linear [positive], linear [negative], nonmonotonic [U], and nonmonotonic [inverted U]) were noted within "Edwardian" strata, demonstrating that although the nonmonotonic functions do exist, they are predominantly associated with moderate Edwards items (e.g., yes these functions do occur but perhaps the ambivalence is also associated with aggregate moderation). Figure 2 presents the relationship between: 1) the functional slope relating an item response's rated level of desirability and the "location" of the rating, and 2) Edwards' item stem rating (on the y-axis). This strong relationship (*r* = -.76) suggests some level of similarity across procedures. 

Undoubtedly, the @kuncel_conceptual_2009 procedure conveys information not contained in the classic @edwards_social_1957 approach. The purpose of this investigation, however, was to document overlap between the two procedures. While it is clear nonmonotonic functions do exist for some indicators across scaled "trait levels," the vast majority of such circumstances are located within a range what the @edwards_social_1957 procedure labels as merely moderately desirable or undesirable. There is surely additional information contained within these items, but the current investigation suggests that perhaps the more cognitively taxing and time-intensive procedure should be retained only for the items first identified by the cognitively-easier and less time-consuming @edwards_social_1957 method. Our recommendation is to therefore retain both procedures, utilizing the cognitively easier and less time-consuming procedure as an initial evaluation and following-up with moderately desirable items to probe for more complex relationships.

Certainly the @kuncel_conceptual_2009 procedure conveys information not contained in the classic @edwards_social_1957 approach. The purpose of this investigation, however, was to document overlap between the two procedures. Clearly nonmonotonic functions do exist for some indicators across scaled "trait levels". However, the vast majority of such circumstances are located within what the @edwards_social_1957 procedure labels as "moderately desirable". There is certainly additional information contained within these items, but the current investigation suggests that perhaps the more cognitively taxing and time-intensive procedure be retained for only those items first identified by the cognitively-easier and less time consuming @edwards_social_1957 method as "moderately desirable". 

@kuncel_conceptual_2009 proposed 4 functional types - visual inspection of our empirical functions suggest that additional information lies in the *location* of the function as well. Witches hat has different implications than egyptian or captain hook. Captain hook and witches hat are both "non-monotonic", but differ in "high point" location, either at neutral or moderately desirable locations. Captain hook can likely be treated as effectively linear whereas witches hat should not. Our functional slope index captures these deviations and should perhaps be considered in future investigations (intercept too? If it's witches hat then intercept *most likely* neutral-ish although not necessarily so).

## Limitations

Our task was likely too long - in retrospect a shorter measure should have been pursued.

> **Redundant with previous paragraph** $\rightarrow$ In order to capture the extremity of function across @kuncel_conceptual_2009 values, several regressions were fit using the average (across respondents) rating as a predictor (e.g., @kuncel_conceptual_2009's "Lower 1%", "Lower 30%", "Median", "Upper 30%", and "Upper 1%" were treated as a scaled continuum) and average Edwards' desirability rating as the criterion. Slopes were retained for each function, with the expectation that slope magnitude and valence would parallel the classic Edwards ratings. 

Our analytical methodology is atypical - the focus of @kuncel_conceptual_2009 was visual functions. Our approach therefore also focused on these functions - extracting regression coefficients as an (imperfect) empirical index. Follow-up analyses retained slope coefficients as meaningful representations of the @kuncel_conceptual_2009 functions. Some of our descriptive correlations are essentially correlations of "correlations", and distal indices such as these are not ideal (for example, for anticipated replication). We chose to retain and present these analyses as descriptive, but acknowledge that alternative methodologies should be pursued in future investigations.  


>Commentary at the end of the references: rated the visual functions along dimensions of "on the whole, this looks like a straight line" with possible ratings ranging from (1 = not at all, to 5 = definitely), and how much the "line rises and falls" from (1 = not at all, to 5 = a lot). These estimates were added to the first approach (defining each Edwards/Kuncel and Tellegen convergence with ratings of both functional linearity and monotonicity) and the result is presented in Figure \@ref(fig:lastone). Careful inspection of the Figure \@ref(fig:lastone) plot again highlights the location of non-monotonic and nonlinear Kuncel and Tellegen functions - predominantly at moderate (around neutral) Edwards rating locations. [NOTE - LOOKS LIKE WE MAY NEED TO REDO THESE ESTIMATES; THERE ARE 96 "NOT AT ALL LOOKS LIKE A STRAIGHT LINE" and 130 "NOT AT ALL RISES AND FALLS"; ALTERNATIVELY GET MORE CREATIVE WITH RESIDUALS ANALYSES (ON DESKTOP)]

>NOTE. Maybe do something with residuals to replace current subjective rating graph - lm object is named, "try" 8/12/23

# References

```{r create_r-references}

r_refs(file="temp.bib")

```


\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup


